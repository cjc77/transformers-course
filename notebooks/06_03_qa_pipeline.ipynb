{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5fb7c03-09ec-4496-b316-53c033c1e07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79c8179d-1378-430c-aabc-43218490583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"distilbert-base-cased-distilled-squad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dc0aa24-25b7-4d36-b87a-b5a521243135",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answerer = pipeline(\"question-answering\", model=checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcae890-8c86-41bb-811f-c36147e418d5",
   "metadata": {},
   "source": [
    "# Question answering examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0e4b1c2-3cf2-4be8-9e8d-8e0e2fe06314",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "ðŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch, and TensorFlow â€” with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9722166-8950-4580-a3f6-b1e5ebb2edf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.98026043176651,\n",
       " 'start': 78,\n",
       " 'end': 106,\n",
       " 'answer': 'Jax, PyTorch, and TensorFlow'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question1 = \"Which deep learning libraries back ðŸ¤— Transformers?\"\n",
    "question_answerer(question=question1, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f3914f2-c47e-444a-be9c-7dac68bed62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.1688368320465088, 'start': 33, 'end': 38, 'answer': 'three'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question2 = \"Do any deep learning libraries back ðŸ¤— Transformers?\"\n",
    "question_answerer(question=question2, context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0db41b0-0622-43e4-8a24-e8dd640185a2",
   "metadata": {},
   "source": [
    "## Question answering from a long context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f416bdd-ed75-4fdb-9c93-c68a44508853",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_context = \"\"\"\n",
    "ðŸ¤— Transformers: State of the Art NLP\n",
    "\n",
    "ðŸ¤— Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\n",
    "question answering, summarization, translation, text generation and more in over 100 languages.\n",
    "Its aim is to make cutting-edge NLP easier to use for everyone.\n",
    "\n",
    "ðŸ¤— Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\n",
    "then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\n",
    "can be modified to enable quick research experiments.\n",
    "\n",
    "Why should I use transformers?\n",
    "\n",
    "1. Easy-to-use state-of-the-art models:\n",
    "  - High performance on NLU and NLG tasks.\n",
    "  - Low barrier to entry for educators and practitioners.\n",
    "  - Few user-facing abstractions with just three classes to learn.\n",
    "  - A unified API for using all our pretrained models.\n",
    "  - Lower compute costs, smaller carbon footprint:\n",
    "\n",
    "2. Researchers can share trained models instead of always retraining.\n",
    "  - Practitioners can reduce compute time and production costs.\n",
    "  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n",
    "\n",
    "3. Choose the right framework for every part of a model's lifetime:\n",
    "  - Train state-of-the-art models in 3 lines of code.\n",
    "  - Move a single model between TF2.0/PyTorch frameworks at will.\n",
    "  - Seamlessly pick the right framework for training, evaluation and production.\n",
    "\n",
    "4. Easily customize a model or an example to your needs:\n",
    "  - We provide examples for each architecture to reproduce the results published by its original authors.\n",
    "  - Model internals are exposed as consistently as possible.\n",
    "  - Model files can be used independently of the library for quick experiments.\n",
    "\n",
    "ðŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch and TensorFlow â€” with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb92a42a-0640-4a3c-9dcc-ddb5390cf8e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9714871048927307,\n",
       "  'start': 1892,\n",
       "  'end': 1919,\n",
       "  'answer': 'Jax, PyTorch and TensorFlow'},\n",
       " {'score': 0.14949694275856018,\n",
       "  'start': 17,\n",
       "  'end': 37,\n",
       "  'answer': 'State of the Art NLP'},\n",
       " {'score': 0.015565154142677784,\n",
       "  'start': 1892,\n",
       "  'end': 1921,\n",
       "  'answer': 'Jax, PyTorch and TensorFlow â€”'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer(question=question1, context=long_context, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54c4c51f-5220-40fa-87ed-fcb40540cd7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.06686097383499146,\n",
       " 'start': 1815,\n",
       " 'end': 1889,\n",
       " 'answer': 'ðŸ¤— Transformers is backed by the three most popular deep learning libraries'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer(question=question2, context=long_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96653b7-4851-4356-b80b-0f51844b793f",
   "metadata": {},
   "source": [
    "# Question answering with a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8494e368-d429-43db-aee0-140b2f76b0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e4f2727-7544-45b8-b7c7-336477faa770",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(question1, context, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7a1e8a8-2d89-470e-ad5d-a5e1969da488",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43cfe355-e51d-45ff-a08c-c1f636ade757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 67]) torch.Size([1, 67])\n"
     ]
    }
   ],
   "source": [
    "print(start_logits.shape, end_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "92d0db0f-3d1f-4c25-a6bf-e721754d1d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_ids = inputs.sequence_ids()\n",
    "# Mask everything except for context tokens\n",
    "mask = [sid != 1 for sid in sequence_ids]\n",
    "# Keep [CLS] token\n",
    "mask[0] = False\n",
    "mask = torch.tensor(mask).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "730ca7da-8026-42ea-aa74-bc2fe8ff71a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits[mask] = -1e4\n",
    "end_logits[mask] = -1e4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0b3b5f97-7b18-4b0f-9ff3-148d10cbb8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_probs = F.softmax(start_logits, dim=-1).squeeze(0)\n",
    "end_probs = F.softmax(end_logits, dim=-1).squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a026f6-2772-4c10-bbe5-0bd4fef0b12d",
   "metadata": {},
   "source": [
    "## Find the answer\n",
    "Need to be careful not to just take the argmax, since this could lead to start_idx > end_idx. Compute probabilities for all pairs where start_idx > end_idx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4c503ac2-163c-4b6c-a4ef-207fcc27d6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = start_probs.unsqueeze(1) * end_probs.unsqueeze(0)\n",
    "scores = torch.triu(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "09790d64-6cf6-4d5d-a962-87e05e4dcf3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((3, 2), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1f18820c-52a7-4cbf-a960-b0deb74bd4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1576, 1577, 1107])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.ravel().topk(3).indices.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb49d4c5-d7c9-4778-9a57-ed779542f87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_idx = scores.argmax().item()\n",
    "# Which row?\n",
    "start_idx = max_idx // scores.shape[1]\n",
    "# Which col?\n",
    "end_idx = max_idx % scores.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "342e0aff-05f3-442e-8712-58afafed3c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 35\n",
      "0.9802628755569458\n"
     ]
    }
   ],
   "source": [
    "print(start_idx, end_idx)\n",
    "print(scores[start_idx, end_idx].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6721f5a3-e50b-44a0-a6f7-082a4edb4df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_idx(scores, k):\n",
    "    start_end = np.zeros((k, 2), dtype=int)\n",
    "    max_idxs = scores.ravel().topk(k).indices.numpy()\n",
    "    for i, mi in enumerate(max_idxs):\n",
    "        # Row\n",
    "        start_idx = mi // scores.shape[1]\n",
    "        # Col\n",
    "        end_idx = mi % scores.shape[1]\n",
    "        start_end[i, 0] = start_idx\n",
    "        start_end[i, 1] = end_idx\n",
    "    return start_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5c7b17d8-1d18-4966-9baa-2bd513118b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_end_topk = topk_idx(scores, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "63a94bc3-084c-468f-ba0f-65a170acc464",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_with_offsets = tokenizer(question1, context, return_offsets_mapping=True)\n",
    "offsets = inputs_with_offsets.offset_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "75c378e6-106a-43fe-8f76-3bdee53f627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_topk = []\n",
    "for start_end in start_end_topk:\n",
    "    start_idx, end_idx = start_end[0], start_end[1]\n",
    "    start_char, _ = offsets[start_idx]\n",
    "    _, end_char = offsets[end_idx]\n",
    "    answer = context[start_char: end_char]\n",
    "    results_topk.append({\n",
    "        \"answer\": answer,\n",
    "        \"start\": start_char,\n",
    "        \"end\": end_char,\n",
    "        \"score\": scores[start_idx, end_idx].item()\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3a4b95a2-3e22-405c-be28-6d2003c86a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer': 'Jax, PyTorch, and TensorFlow',\n",
       "  'start': 78,\n",
       "  'end': 106,\n",
       "  'score': 0.9802628755569458},\n",
       " {'answer': 'Jax, PyTorch, and TensorFlow â€”',\n",
       "  'start': 78,\n",
       "  'end': 108,\n",
       "  'score': 0.008247802965342999},\n",
       " {'answer': 'three most popular deep learning libraries â€” Jax, PyTorch, and TensorFlow',\n",
       "  'start': 33,\n",
       "  'end': 106,\n",
       "  'score': 0.006841438356786966}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doesn't exactly match the answer predicted by the pipeline...\n",
    "results_topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "62a3439e-5a25-4d38-9cc1-db50af92aed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(question1, long_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c2aaf554-3be3-40b3-8f34-3a3e587c8d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "068bfd75-eeca-4306-a74a-da49decafaff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the demo, this is 384\n",
    "question_answerer.model.config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e919f5c5-58cb-429d-9ac0-12a6537efe2f",
   "metadata": {},
   "source": [
    "# Truncate inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a589e1d3-a0b2-41ae-8801-f7cc45474baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(question1, long_context, max_length=384, truncation=\"only_second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6f48296d-e1a3-437f-b08c-df42ab0b571d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP [UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more in over 100 languages. Its aim is to make cutting - edge NLP easier to use for everyone. [UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine - tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments. Why should I use transformers? 1. Easy - to - use state - of - the - art models : - High performance on NLU and NLG tasks. - Low barrier to entry for educators and practitioners. - Few user - facing abstractions with just three classes to learn. - A unified API for using all our pretrained models. - Lower compute costs, smaller carbon footprint : 2. Researchers can share trained models instead of always retraining. - Practitioners can reduce compute time and production costs. - Dozens of architectures with over 10, 000 pretrained models, some in more than 100 languages. 3. Choose the right framework for every part of a model's lifetime : - Train state - of - the - art models in 3 lines of code. - Move a single model between TF2. 0 / PyTorch frameworks at will. - Seamlessly pick the right framework for training, evaluation and production. 4. Easily customize a model or an example to your needs : - We provide examples for each architecture to reproduce the results published by its original authors. - Model internal [SEP]\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92429d7e-cae3-458c-837f-387d965a2265",
   "metadata": {},
   "source": [
    "## Place overlapping tokens in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "991363d5-88ea-4a0b-9718-30a9dc4cae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"This sentence is not too long but we are going to split it anyway.\",\n",
    "    \"This sentence is shorter but will still get split.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d1b22580-ee15-4015-892d-726ef80628ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    sentences, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "95176373-c8aa-4041-b55b-78740811c2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] This sentence is not [SEP]\n",
      "[CLS] is not too long [SEP]\n",
      "[CLS] too long but we [SEP]\n",
      "[CLS] but we are going [SEP]\n",
      "[CLS] are going to split [SEP]\n",
      "[CLS] to split it anyway [SEP]\n",
      "[CLS] it anyway. [SEP]\n",
      "[CLS] This sentence is shorter [SEP]\n",
      "[CLS] is shorter but will [SEP]\n",
      "[CLS] but will still get [SEP]\n",
      "[CLS] still get split. [SEP]\n"
     ]
    }
   ],
   "source": [
    "for ids in inputs.input_ids:\n",
    "    print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "48b6c0d5-34f9-48a4-85a3-e8f5a82805fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cd7b0b8c-336a-41ed-ad97-7255e0fb5f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.overflow_to_sample_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35079e45-6b8a-4c09-81a7-f5c47011650f",
   "metadata": {},
   "source": [
    "# Redo tokenization using the new tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "211e2b81-c4b0-4942-9d62-b6a17948ea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    question1,\n",
    "    long_context,\n",
    "    stride=128,\n",
    "    max_length=384,\n",
    "    padding=\"longest\",\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "79eece4b-ddd7-4123-97b1-6a72d4c33372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e59480-ce5f-4198-942c-e184035e8272",
   "metadata": {},
   "source": [
    "## Get rid of keys we don't need right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d70ce3f3-78be-448e-8520-ce353154fd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "offsets = inputs.pop(\"offset_mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2e7e9bb9-e8bb-42b2-a4a1-42519a631ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs.convert_to_tensors(\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "aa0cac3a-a6c8-41ca-aa1b-aceced48e68e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 384])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e9b1e7c4-8bcc-4bad-adea-5f159e01a50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d4c7ac7a-7641-4ee7-a1aa-e984ca46a0c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 384])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0a7f3b80-38e4-41f3-b2e2-a937a7ac300f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 384])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e95db30-025d-4c7a-8c28-e75b8883e793",
   "metadata": {},
   "source": [
    "## Mask non-context and padding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "eba82b41-2f14-4347-b940-56ebe65fdbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_ids = inputs.sequence_ids()\n",
    "mask = [sid != 1 for sid in sequence_ids]\n",
    "mask[0] = False\n",
    "mask = torch.logical_or(torch.tensor(mask)[None, :], (inputs.attention_mask == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "aefaba27-cf22-4e9f-8326-ac0eaf82fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits[mask] = -1e4\n",
    "end_logits[mask] = -1e4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "aabd92e2-3a78-4805-9727-7d05d2c41c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_probs = F.softmax(start_logits, dim=-1)\n",
    "end_probs = F.softmax(end_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a5174b-3aef-446f-99cf-8e5ba632db2f",
   "metadata": {},
   "source": [
    "## Find span with highest probability score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "29e8c3b4-09b2-4cc6-bf98-96570ffcd95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = []\n",
    "for start_p, end_p in zip(start_probs, end_probs):\n",
    "    scores = start_p[:, None] * end_p[None, :]\n",
    "    idx = torch.triu(scores).argmax().item()\n",
    "\n",
    "    start_idx = idx // scores.shape[1]\n",
    "    end_idx = idx % scores.shape[1]\n",
    "    score = scores[start_idx, end_idx].item()\n",
    "    candidates.append((start_idx, end_idx, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a2313b23-0217-4421-9910-dbfd90fa163b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 18, 0.3386707901954651), (173, 184, 0.9714868664741516)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "313e6195-bc08-4cc9-9b1a-b0135131c579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': '\\nðŸ¤— Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.3386707901954651}\n",
      "{'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.9714868664741516}\n"
     ]
    }
   ],
   "source": [
    "for candidate, offset in zip(candidates, offsets):\n",
    "    start_token, end_token, score = candidate\n",
    "    start_char, _ = offset[start_token]\n",
    "    _, end_char = offset[end_token]\n",
    "    answer = long_context[start_char: end_char]\n",
    "    result = {\n",
    "        \"answer\": answer,\n",
    "        \"start\": start_char,\n",
    "        \"end\": end_char,\n",
    "        \"score\": score,\n",
    "    }\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d88fa46-0a56-41ea-94a0-06020c6cccf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
