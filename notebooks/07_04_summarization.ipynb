{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "779f1385-e760-495d-a740-b155ee2c2828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import evaluate\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from accelerate import Accelerator\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    get_scheduler,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2ca5c7-9238-4bba-bdba-47b1223a3117",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e857e3c1-387e-40e8-8cb5-53bfca2602b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_checkpoint = \"amazon_reviews_multi\"\n",
    "dataset_commit_id = \"f256e74ee2353b7c7854f86f86200f220531caa4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cd954d8-b4b0-4429-bc74-ceae77a17f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_dataset = load_dataset(dataset_checkpoint, revision=dataset_commit_id, name=\"es\")\n",
    "english_dataset = load_dataset(dataset_checkpoint, revision=dataset_commit_id, name=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0afe558-dc34-4d5a-93c1-5c5c7ba7ff9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 200000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ab9ef0e-bd43-43bf-8812-2104e0cc6c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 200000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaaa04a-d279-43e5-880d-16f6c0fdef85",
   "metadata": {},
   "source": [
    "## Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b802bfc3-03d0-404b-bc04-7b5203f696be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples(dataset, num_samples=3, seed=42):\n",
    "    sample = dataset[\"train\"].shuffle(seed=seed).select(range(num_samples))\n",
    "    for example in sample:\n",
    "        print(f\"\\n>> Title: {example['review_title']}\")\n",
    "        print(f\">> Review: '{example['review_body']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfb94273-cb91-4c46-bb0f-7d53af61bd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Title: Worked in front position, not rear\n",
      ">> Review: '3 stars because these are not rear brakes as stated in the item description. At least the mount adapter only worked on the front fork of the bike that I got it for.'\n",
      "\n",
      ">> Title: meh\n",
      ">> Review: 'Does it’s job and it’s gorgeous but mine is falling apart, I had to basically put it together again with hot glue'\n",
      "\n",
      ">> Title: Can't beat these for the money\n",
      ">> Review: 'Bought this for handling miscellaneous aircraft parts and hanger \"stuff\" that I needed to organize; it really fit the bill. The unit arrived quickly, was well packaged and arrived intact (always a good sign). There are five wall mounts-- three on the top and two on the bottom. I wanted to mount it on the wall, so all I had to do was to remove the top two layers of plastic drawers, as well as the bottom corner drawers, place it when I wanted and mark it; I then used some of the new plastic screw in wall anchors (the 50 pound variety) and it easily mounted to the wall. Some have remarked that they wanted dividers for the drawers, and that they made those. Good idea. My application was that I needed something that I can see the contents at about eye level, so I wanted the fuller-sized drawers. I also like that these are the new plastic that doesn't get brittle and split like my older plastic drawers did. I like the all-plastic construction. It's heavy duty enough to hold metal parts, but being made of plastic it's not as heavy as a metal frame, so you can easily mount it to the wall and still load it up with heavy stuff, or light stuff. No problem there. For the money, you can't beat it. Best one of these I've bought to date-- and I've been using some version of these for over forty years.'\n"
     ]
    }
   ],
   "source": [
    "show_samples(english_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "003ae80e-7dee-4f87-9aac-810145ea73cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Title: .\n",
      ">> Review: 'La montarlo se rompió una rueda debido a materiales débiles, pero al arreglarla funciona correctamente.'\n",
      "\n",
      ">> Title: Primeras impresiones\n",
      ">> Review: 'El servicio ha sido muy bueno, me ha llegado 2 días antes de lo previsto. En cuanto al producto no es que me haya dado muy buenas primeras impresiones. El borde del protector es de plástico y lo único que hay de cristal es la pantalla. Además el plástico es muy fino. A nivel estético queda muy bien y se ajusta perfectamente, la única queja que tengo es eso, que no sea todo de cristal y que para mi gusto es demasiado fino. De la resistencia no tengo ni idea ya que es el primer día que lo llevo. No creo que sea mal producto del todo si no que depende del gusto y el cuidado que tenga cada uno de su móvil. Personalmente creo que por el mismo precio hay otros productos que si que son enteros de cristal y más gordos que por lo menos a mí me generan más confianza.'\n",
      "\n",
      ">> Title: .\n",
      ">> Review: 'Funciona genial y la llevo conmigo en todas las ocasiones importantes. Estoy muy muy contenta con esta compra, la verdad.'\n"
     ]
    }
   ],
   "source": [
    "show_samples(spanish_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "295086c5-d4c9-405a-8b87-963e7f5c335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_dataset.set_format(\"pandas\")\n",
    "english_df = english_dataset[\"train\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db2279ef-14ee-4915-8d9d-e8698760bbba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_category\n",
       "home                        17679\n",
       "apparel                     15951\n",
       "wireless                    15717\n",
       "other                       13418\n",
       "beauty                      12091\n",
       "drugstore                   11730\n",
       "kitchen                     10382\n",
       "toy                          8745\n",
       "sports                       8277\n",
       "automotive                   7506\n",
       "lawn_and_garden              7327\n",
       "home_improvement             7136\n",
       "pet_products                 7082\n",
       "digital_ebook_purchase       6749\n",
       "pc                           6401\n",
       "electronics                  6186\n",
       "office_product               5521\n",
       "shoes                        5197\n",
       "grocery                      4730\n",
       "book                         3756\n",
       "baby_product                 3150\n",
       "furniture                    2984\n",
       "jewelry                      2747\n",
       "camera                       2139\n",
       "industrial_supplies          1994\n",
       "digital_video_download       1364\n",
       "luggage                      1328\n",
       "musical_instruments          1102\n",
       "video_games                   775\n",
       "watch                         761\n",
       "personal_care_appliances       75\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_df[\"product_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf9ab95-eae8-4de0-a94d-0c49fc2e9e9f",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0366373-4423-4e1b-83f5-a4f4848e0181",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_dataset.reset_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31725873-2c51-43e8-9425-934069f06125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_books(example):\n",
    "    return (\n",
    "        example[\"product_category\"] == \"book\"\n",
    "        or example[\"product_category\"] == \"digital_ebook_purchase\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "953510e2-d9a3-40ed-9638-e820ff65aa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_books = english_dataset.filter(filter_books)\n",
    "spanish_books = spanish_dataset.filter(filter_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5db1cabc-7912-4912-b048-35ef4702f91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Title: I'm dissapointed.\n",
      ">> Review: 'I guess I had higher expectations for this book from the reviews. I really thought I'd at least like it. The plot idea was great. I loved Ash but, it just didnt go anywhere. Most of the book was about their radio show and talking to callers. I wanted the author to dig deeper so we could really get to know the characters. All we know about Grace is that she is attractive looking, Latino and is kind of a brat. I'm dissapointed.'\n",
      "\n",
      ">> Title: Good art, good price, poor design\n",
      ">> Review: 'I had gotten the DC Vintage calendar the past two years, but it was on backorder forever this year and I saw they had shrunk the dimensions for no good reason. This one has good art choices but the design has the fold going through the picture, so it's less aesthetically pleasing, especially if you want to keep a picture to hang. For the price, a good calendar'\n",
      "\n",
      ">> Title: Helpful\n",
      ">> Review: 'Nearly all the tips useful and. I consider myself an intermediate to advanced user of OneNote. I would highly recommend.'\n"
     ]
    }
   ],
   "source": [
    "show_samples(english_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f871446-1151-4ef8-a1e3-9deaeae36f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_dataset = DatasetDict()\n",
    "\n",
    "for split in english_books.keys():\n",
    "    books_dataset[split] = concatenate_datasets(\n",
    "        [english_books[split], spanish_books[split]]\n",
    "    )\n",
    "    books_dataset[split] = books_dataset[split].shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e0d945e-6857-4db2-a706-aecddc3564cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Title: Easy to follow!!!!\n",
      ">> Review: 'I loved The dash diet weight loss Solution. Never hungry. I would recommend this diet. Also the menus are well rounded. Try it. Has lots of the information need thanks.'\n",
      "\n",
      ">> Title: PARCIALMENTE DAÑADO\n",
      ">> Review: 'Me llegó el día que tocaba, junto a otros libros que pedí, pero la caja llegó en mal estado lo cual dañó las esquinas de los libros porque venían sin protección (forro).'\n",
      "\n",
      ">> Title: no lo he podido descargar\n",
      ">> Review: 'igual que el anterior'\n"
     ]
    }
   ],
   "source": [
    "show_samples(books_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fca7ec-9753-40ec-a856-0c86aedb86ff",
   "metadata": {},
   "source": [
    "## TODO: make word count distribution plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba7b750-9293-41f4-bc80-41858b19f65d",
   "metadata": {},
   "source": [
    "## Filter low-wordcount review titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1158d434-4f37-48eb-899e-72c411dbcb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use white space heuristic for \"word count\"\n",
    "books_dataset = books_dataset.filter(lambda x: len(x[\"review_title\"].split()) > 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa971d5-d241-4f57-998d-a09421da25df",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3b29dd8-5b58-498b-acac-5cff37005d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"google/mt5-small\"\n",
    "model_commit_id = \"38f23af8ec210eb6c376d40e9c56bd25a80f195d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94cd96b6-13cf-489c-9a6e-e72d1faa2f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "/home/carcook/dev/transformers-course/myenv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, revision_id=model_commit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e282d205-e56c-4ab0-9f5d-b3cf97b9bf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"I think Catch 22 is the best book ever.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b328a84-2e1f-4562-b427-2fdf1de0d52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [336, 5231, 259, 139068, 1024, 339, 287, 1920, 3435, 14049, 260, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e165613c-8966-4775-a676-4c6f1253cb04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁I',\n",
       " '▁think',\n",
       " '▁',\n",
       " 'Catch',\n",
       " '▁22',\n",
       " '▁is',\n",
       " '▁the',\n",
       " '▁best',\n",
       " '▁book',\n",
       " '▁ever',\n",
       " '.',\n",
       " '</s>']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "222264d4-a3e9-4e18-bebb-45eabcefea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 512\n",
    "max_target_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "898cf6f2-ceb5-4a64-b473-ba251e873bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"review_body\"],\n",
    "        max_length=max_input_length,\n",
    "        truncation=True\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"review_title\"],\n",
    "        max_length=max_target_length,\n",
    "        truncation=True\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5c16ef6-f358-4b4b-93a6-44465b874f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = books_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a847816-4019-4198-ad4b-5b8df354965d",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cc2a61a-a45c-4f12-a0f3-8e656aada1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_summary = \"I absolutely loved reading Catch 22\"\n",
    "reference_summary = \"I loved reading Catch 22\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ae80499-788c-4020-919f-4cb18e33203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_score = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6725c6fa-c2e9-4d7f-82cc-c362c1614720",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = rouge_score.compute(\n",
    "    predictions=[generated_summary],\n",
    "    references=[reference_summary],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "108dd9ac-cb22-4f8a-831b-ae126d2104b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.9090909090909091,\n",
       " 'rouge2': 0.6666666666666665,\n",
       " 'rougeL': 0.9090909090909091,\n",
       " 'rougeLsum': 0.9090909090909091}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a11b10-4a49-44d9-8ebf-eb9c34e3fe49",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc9802b3-1ae7-40f5-8160-ab0ce5b3026d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/carcook/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44bc5fbd-4d01-435e-a6e7-d0ccc8597df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_sentence_summary(text):\n",
    "    return \"\\n\".join(sent_tokenize(text)[:3])\n",
    "\n",
    "\n",
    "def evaluate_baseline(dataset, metric):\n",
    "    summaries = [three_sentence_summary(text) for text in dataset[\"review_body\"]]\n",
    "    return metric.compute(predictions=summaries, references=dataset[\"review_title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49c8cc7b-27e9-4f82-aa03-690b47d1c756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I grew up reading Koontz, and years ago, I stopped,convinced i had \"outgrown\" him.\\nStill,when a friend was looking for something suspenseful too read, I suggested Koontz.\\nShe found Strangers.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_sentence_summary(books_dataset[\"train\"][1][\"review_body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29e1a718-740a-4fda-814b-1fea80e5ac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = evaluate_baseline(books_dataset[\"validation\"], rouge_score)\n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "rouge_dict = dict(\n",
    "    (rn, round(score[rn] * 100, 2)) for rn in rouge_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89f83561-4aba-406f-be6c-19afba1ab494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 16.76, 'rouge2': 8.9, 'rougeL': 15.52, 'rougeLsum': 15.97}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a29122-e1fe-4fe6-b4f6-b016f5c909e7",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f833dd8-25f3-4cb8-85f4-84358630c4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, revision=model_commit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5104b603-8d73-4e54-9610-01542d431afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_train_epochs = 8\n",
    "logging_steps = len(tokenized_datasets[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "output_dir = f\"../temp/07/{model_name}-finetuned-amazon-en-es\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a897276b-a2b1-42aa-9b21-14db0afa265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5.6e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=logging_steps,\n",
    "    push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51a3230f-aba2-466e-a52b-4e112025614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100s, which can't be decoded\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Prep for ROUGE\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    # ROUGE scores\n",
    "    result = rouge_score.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "    # Extract the median scores\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "009bb9f7-aa11-4afa-9d33-bd59164dd605",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "18615c6c-2233-4dc9-a60c-d7553acb8c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns(\n",
    "    books_dataset[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec17138a-929f-410d-a880-444ebc46ba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [tokenized_datasets[\"train\"][i] for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3bce1017-9d06-4fe7-916f-913efe9dd69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   653,   1957,   1314,    261,   2757,   1280,    435,    259,  29166,\n",
       "            263,    269,    774,   5547,      1,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0],\n",
       "        [   336,   6998,    481,   1150,  11807,   1590,  16339,    360,    261,\n",
       "            305,   3127,   2780,    261,    336,  35042,    345,    261,   3141,\n",
       "         136052,    285,    259,    266,   1425,    313,   2983, 106419,    272,\n",
       "            311,   4065,    260,  36874,    261,   1909,    259,    262,  22163,\n",
       "            639,    259,   7505,    332,   9066,  88398,    265,   5105,   6320,\n",
       "           4906,    261,    336,  21847,    345,   1590,  16339,    360,    260,\n",
       "           4630,   5897,    320, 146261,    260,    486,  21101,   1998,   3684,\n",
       "           2606,   2316,    609,    639,   3014,  11665,    416,    260,    336,\n",
       "            639,   7779,    259,    266,   1425,  57512,   9066,   5401,    260,\n",
       "            336,    259,  91451,    259,    262,  27613,    332,    259,  34779,\n",
       "            260,   1494,    639,    259,    262,   3005,    584,  62351,    288,\n",
       "            461,   6801,  22590,  62799,   3093,    259,    262,  20233,   3622,\n",
       "            304,    259,  11994,    259,  36260,  48971,    533,    418,   2725,\n",
       "            484,  27067,   1059,    260,  18646,    521,    259,   3659,   1063,\n",
       "           1388,    514,   3622,   3004,  26222,    260,    298, 109508,    276,\n",
       "            533,   1425,    288,    390,    259, 131690,    260,    336,    259,\n",
       "          25505,    609,    288,    390,    287,   3435,    336,   4906,   6338,\n",
       "            259,   6924,    259,  29426,    261,   2469,    351,    259, 109933,\n",
       "            260,    563,  15129,    261,    336,    277,    857,  36975,    345,\n",
       "          14355,    305,    259,    265, 111962,    288,  27039,    714,    260,\n",
       "            459,  25640,    259,    262,   5150,   4906,    260,      1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  298,   259,  5994,   269,   774,  5547,     1],\n",
       "        [  298, 10380,   304, 13992,   291,     1,  -100]]), 'decoder_input_ids': tensor([[    0,   298,   259,  5994,   269,   774,  5547],\n",
       "        [    0,   298, 10380,   304, 13992,   291,     1]])}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8602912-0a90-45c3-8d4c-8f94f19dd714",
   "metadata": {},
   "source": [
    "## Subsample to ensure manageable computation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a986f0ac-fcf5-4431-bf5f-2639c2737fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_samp = int(0.4 * tokenized_datasets[\"train\"].num_rows)\n",
    "n_eval_samp = int(0.4 * tokenized_datasets[\"validation\"].num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0aa20f4a-58d8-4188-bc3c-7300ea8cc810",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"].shuffle(seed=42).select(range(n_train_samp)),\n",
    "    eval_dataset=tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(n_eval_samp)),\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5682438c-a1dd-4fde-bb8a-0bd476ad0d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carcook/dev/transformers-course/myenv/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3872' max='3872' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3872/3872 20:33, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.668645</td>\n",
       "      <td>7.120500</td>\n",
       "      <td>1.691000</td>\n",
       "      <td>7.017100</td>\n",
       "      <td>6.959100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.386395</td>\n",
       "      <td>14.620200</td>\n",
       "      <td>6.733500</td>\n",
       "      <td>13.900300</td>\n",
       "      <td>13.864600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.821700</td>\n",
       "      <td>3.357860</td>\n",
       "      <td>15.318200</td>\n",
       "      <td>6.392300</td>\n",
       "      <td>14.201500</td>\n",
       "      <td>14.157800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.821700</td>\n",
       "      <td>3.328821</td>\n",
       "      <td>15.838500</td>\n",
       "      <td>7.906600</td>\n",
       "      <td>15.250600</td>\n",
       "      <td>15.371300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.887500</td>\n",
       "      <td>3.307375</td>\n",
       "      <td>14.776300</td>\n",
       "      <td>6.698300</td>\n",
       "      <td>14.509200</td>\n",
       "      <td>14.413100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.887500</td>\n",
       "      <td>3.291033</td>\n",
       "      <td>17.387200</td>\n",
       "      <td>9.190500</td>\n",
       "      <td>16.808100</td>\n",
       "      <td>16.804200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.887500</td>\n",
       "      <td>3.281902</td>\n",
       "      <td>17.053100</td>\n",
       "      <td>8.690000</td>\n",
       "      <td>16.479600</td>\n",
       "      <td>16.612600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.629200</td>\n",
       "      <td>3.285086</td>\n",
       "      <td>17.319800</td>\n",
       "      <td>8.972700</td>\n",
       "      <td>16.852200</td>\n",
       "      <td>16.870500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3872, training_loss=4.701760552146218, metrics={'train_runtime': 1234.0482, 'train_samples_per_second': 25.075, 'train_steps_per_second': 3.138, 'total_flos': 4861981464207360.0, 'train_loss': 4.701760552146218, 'epoch': 8.0})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4445ea8e-fc18-4d27-a6de-62b1200c5945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.285085678100586,\n",
       " 'eval_rouge1': 17.3198,\n",
       " 'eval_rouge2': 8.9727,\n",
       " 'eval_rougeL': 16.8522,\n",
       " 'eval_rougeLsum': 16.8705,\n",
       " 'eval_runtime': 4.8239,\n",
       " 'eval_samples_per_second': 19.694,\n",
       " 'eval_steps_per_second': 2.488,\n",
       " 'epoch': 8.0}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eb7339-99e9-4b39-8189-a00a9bf92683",
   "metadata": {},
   "source": [
    "# Train and evaluate model with Accelerate & custom loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0962eb-fbc3-4526-ade4-6599990da0ec",
   "metadata": {},
   "source": [
    "## DataLoaders and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6ef2f164-6e41-4b69-8178-472a0dabd9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9e1405a0-e439-4c6f-9bdc-22ff2a87e3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5a25d455-eb43-4239-985f-75c0a16c01e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"].shuffle(seed=42).select(range(n_train_samp)),\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(n_eval_samp)),\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415bb490-66c6-421f-a605-f8748af52a37",
   "metadata": {},
   "source": [
    "## Other components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b7969518-8fb4-4f47-85ae-4e13c033e722",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dab954cd-4071-4ce1-b1de-5055fc90be05",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 10\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "73087771-da2a-4e5c-8f85-1bcc7f9ede2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "93c7d218-865d-462f-92e6-8b77d8b8ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # ROUGE expect newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da5ad9d-ce24-450c-9e53-acab12e07e36",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e0a5d756-7a53-49c3-83a1-ca44c6d45b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_accel = f\"{output_dir}-accelerate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6b0c95c5-bf5b-4e61-9356-5ef8ff09e221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8cf95e979944bf795d05ffb6b5c146d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4840 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carcook/dev/transformers-course/myenv/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: {'rouge1': 0.7866, 'rouge2': 0.3828, 'rougeL': 0.7866, 'rougeLsum': 0.7866}\n",
      "Epoch 1: {'rouge1': 1.1077, 'rouge2': 0.0, 'rougeL': 1.0424, 'rougeLsum': 1.0577}\n",
      "Epoch 2: {'rouge1': 1.4812, 'rouge2': 0.4921, 'rougeL': 1.4719, 'rougeLsum': 1.4709}\n",
      "Epoch 3: {'rouge1': 1.9814, 'rouge2': 0.4721, 'rougeL': 1.9208, 'rougeLsum': 1.9258}\n",
      "Epoch 4: {'rouge1': 2.1683, 'rouge2': 0.1914, 'rougeL': 1.9338, 'rougeLsum': 1.9285}\n",
      "Epoch 5: {'rouge1': 3.6431, 'rouge2': 0.8038, 'rougeL': 3.4985, 'rougeLsum': 3.4505}\n",
      "Epoch 6: {'rouge1': 3.8504, 'rouge2': 0.6325, 'rougeL': 3.6319, 'rougeLsum': 3.5949}\n",
      "Epoch 7: {'rouge1': 4.3953, 'rouge2': 0.8038, 'rougeL': 4.336, 'rougeLsum': 4.3013}\n",
      "Epoch 8: {'rouge1': 4.5617, 'rouge2': 0.9467, 'rougeL': 4.5015, 'rougeLsum': 4.4963}\n",
      "Epoch 9: {'rouge1': 4.4103, 'rouge2': 0.9467, 'rougeL': 4.2996, 'rougeLsum': 4.2964}\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Train\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "            )\n",
    "\n",
    "            generated_tokens = accelerator.pad_across_processes(\n",
    "                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "            )\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            labels = accelerator.pad_across_processes(\n",
    "                batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n",
    "            labels = accelerator.gather(labels).cpu().numpy()\n",
    "\n",
    "            # Replace -100 in the labels as we can't decode them\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            if isinstance(generated_tokens, tuple):\n",
    "                generated_tokens = generated_tokens[0]\n",
    "            decoded_preds = tokenizer.batch_decode(\n",
    "                generated_tokens, skip_special_tokens=True\n",
    "            )\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            decoded_preds, decoded_labels = postprocess_text(\n",
    "                decoded_preds, decoded_labels\n",
    "            )\n",
    "\n",
    "            rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    result = rouge_score.compute()\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    print(f\"Epoch {epoch}:\", result)\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir_accel, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir_accel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1158105-9ae9-4856-bb70-400f14e352f8",
   "metadata": {},
   "source": [
    "# Predictions with fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9bfe4232-259b-406c-9e1b-0751527c2002",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_model = sorted(os.listdir(output_dir))[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "db9ee977-3cd8-4e15-98ba-8198942bf261",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=f\"{output_dir}/{last_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b8a42db3-b089-4cce-802d-0b94c238e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(idx):\n",
    "    review = books_dataset[\"test\"][idx][\"review_body\"]\n",
    "    title = books_dataset[\"test\"][idx][\"review_title\"]\n",
    "    summary = summarizer(books_dataset[\"test\"][idx][\"review_body\"])[0][\"summary_text\"]\n",
    "    print(f\">>> Title: '{title}'\")\n",
    "    print(f\"\\n>>> Summary: '{summary}'\")\n",
    "    print(f\"\\n>>> Review: '{review}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5d31646e-9d1c-499c-b579-1c53bf6e769b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Title: 'Opinion on fangirl'\n",
      "\n",
      ">>> Summary: 'It was a whirlwind'\n",
      "\n",
      ">>> Review: 'This novel took my heart. If you’ve read Eleanor and Park you know your in for a whirlwind. It was everything I expected.'\n"
     ]
    }
   ],
   "source": [
    "print_summary(45)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
