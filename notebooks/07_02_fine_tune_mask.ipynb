{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "31ab5bd2-850c-4d99-b06d-366e3fc42f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "from multiprocessing import cpu_count\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    default_data_collator,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    get_scheduler,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87c35600-c18b-45db-8093-97ae9cfe712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "# Commit ID at time of executing this notebook\n",
    "model_commit_id = \"6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22f2a444-8902-4be0-8c2c-6ef7fb56f9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint, revision=model_commit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "525eabf1-5fee-4904-935c-f4d5da067eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tgt_cores = max(1, cpu_count() - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697ba3c6-cc5d-4929-a59d-bb5e2e226c49",
   "metadata": {},
   "source": [
    "# Off-the-shelf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3d3e00b-6a76-4f8e-9e2a-52d53f239f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_num_params = model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c3ff7db-3af6-4bbe-a8aa-5dc1c84f8e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67M parameters\n"
     ]
    }
   ],
   "source": [
    "print(f\"{round(distilbert_num_params / 1e6)}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9741432c-c96b-4f6c-a54f-d34ac72006e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, revision=model_commit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a87f3cd4-9369-4458-934e-467313b94ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a great [MASK].\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "984bcb20-eee0-46fb-b417-562f2472df3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    token_logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ace7a03a-302b-4f72-afd3-e86b5519f080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 30522])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "28834d72-b62e-4876-b171-1e55a2a7a83b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] this is a great [MASK]. [SEP]'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs.input_ids.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2451e16b-d13e-43cf-a225-4e466b830fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'this', 'is', 'a', 'great', '[MASK]', '.', '[SEP]']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs.input_ids.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "91508286-54d7-4197-9c6f-2591781a0a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    mask_token_idx = torch.where(inputs.input_ids == tokenizer.mask_token_id)[1]\n",
    "    mask_token_logits = token_logits.squeeze(0)[mask_token_idx, :]\n",
    "# Choose top 5 candidates for [MASK]\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8abd55d7-b2ad-4157-ab64-1ccc498b1556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: This is a great deal.\n",
      "1: This is a great success.\n",
      "2: This is a great adventure.\n",
      "3: This is a great idea.\n",
      "4: This is a great feat.\n"
     ]
    }
   ],
   "source": [
    "for i, token in enumerate(top_5_tokens):\n",
    "    print(f\"{i}: {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47271c94-8f93-491c-a67b-1a2f9a32944a",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dbb5ff99-d6ee-44d1-b2ac-b55d4c3500ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_checkpoint = \"imdb\"\n",
    "dataset_commit_id = \"9c6ede893febf99215a29cc7b72992bb1138b06b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a69a5322-a5fd-4a3a-a02c-eccd50573c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dataset = load_dataset(dataset_checkpoint, revision=dataset_commit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e4d693e-39fc-4ee9-a6ab-906dac4731f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "55498d15-ded0-4170-856b-232038576134",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = imdb_dataset[\"train\"].shuffle(seed=42).select(range(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1da54e54-08cd-415e-b23a-0a2eaa38ce1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Review: There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier's plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it's the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...\n",
      ">>> Label: 1\n",
      "\n",
      ">>> Review: This movie is a great. The plot is very true to the book which is a classic written by Mark Twain. The movie starts of with a scene where Hank sings a song with a bunch of kids called \"when you stub your toe on the moon\" It reminds me of Sinatra's song High Hopes, it is fun and inspirational. The Music is great throughout and my favorite song is sung by the King, Hank (bing Crosby) and Sir \"Saggy\" Sagamore. OVerall a great family movie or even a great Date movie. This is a movie you can watch over and over again. The princess played by Rhonda Fleming is gorgeous. I love this movie!! If you liked Danny Kaye in the Court Jester then you will definitely like this movie.\n",
      ">>> Label: 1\n",
      "\n",
      ">>> Review: George P. Cosmatos' \"Rambo: First Blood Part II\" is pure wish-fulfillment. The United States clearly didn't win the war in Vietnam. They caused damage to this country beyond the imaginable and this movie continues the fairy story of the oh-so innocent soldiers. The only bad guys were the leaders of the nation, who made this war happen. The character of Rambo is perfect to notice this. He is extremely patriotic, bemoans that US-Americans didn't appreciate and celebrate the achievements of the single soldier, but has nothing but distrust for leading officers and politicians. Like every film that defends the war (e.g. \"We Were Soldiers\") also this one avoids the need to give a comprehensible reason for the engagement in South Asia. And for that matter also the reason for every single US-American soldier that was there. Instead, Rambo gets to take revenge for the wounds of a whole nation. It would have been better to work on how to deal with the memories, rather than suppressing them. \"Do we get to win this time?\" Yes, you do.\n",
      ">>> Label: 0\n"
     ]
    }
   ],
   "source": [
    "for i, row in enumerate(sample):\n",
    "    gap = \"\"\n",
    "    if i != 0:\n",
    "        gap = '\\n'\n",
    "    print(f\"{gap}>>> Review: {row['text']}\")\n",
    "    print(f\">>> Label: {row['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239ed7db-b1c9-4c2f-9f4a-2e7aae0f672a",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "44fab258-95fc-4740-b2f8-c5fa7d4d6447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    results = tokenizer(examples[\"text\"])\n",
    "    n_examples = len(results[\"input_ids\"])\n",
    "    if tokenizer.is_fast:\n",
    "        # Match up tokens with the corresponding word in the original input\n",
    "        results[\"word_ids\"] = [results.word_ids(e) for e in range(n_examples)]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "302faea2-6c49-442c-a1ee-44638909426e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a77d56cded943658950d7f5454f0fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad960fd65fd946819b0117a5a664c59a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe6bf9010c1483593180101f26d6278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = imdb_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\", \"label\"],\n",
    "    num_proc=num_tgt_cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cb81104b-f92c-4682-9a7d-029f52eca79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fc99d558-12e3-4765-8412-99987b02e2f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0559ee5-f183-4c42-8bc8-cfa22c277c64",
   "metadata": {},
   "source": [
    "## Chunk data before tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "727d6869-65a4-4cc9-a849-a4aa30ad981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e64d9193-f87e-43e9-9bf4-12689134e43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_samples = tokenized_datasets[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5daafcb0-d5fc-47b1-aa5b-7a982fc599d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Review 0 length: 363\n",
      ">>> Review 1 length: 304\n",
      ">>> Review 2 length: 133\n"
     ]
    }
   ],
   "source": [
    "for i, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\">>> Review {i} length: {len(sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7d6b85f2-135b-4612-926a-6563f3427b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all examples\n",
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "022c75a6-ea3b-49bd-8ca4-8e38b3a769d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Concatenated reviews length: 800\n"
     ]
    }
   ],
   "source": [
    "print(f\">>> Concatenated reviews length: {total_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "72c8c581-0320-47b7-b5ab-1ee1804cbb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into chunks by 'block size'\n",
    "chunks = {\n",
    "    k: [t[i:i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e3a5013d-190e-4d7d-8d88-d7434e29a436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Chunk length: 128\n",
      ">>> Chunk length: 128\n",
      ">>> Chunk length: 128\n",
      ">>> Chunk length: 128\n",
      ">>> Chunk length: 128\n",
      ">>> Chunk length: 128\n",
      ">>> Chunk length: 32\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\">>> Chunk length: {len(chunk)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3885ad14-5c48-4e99-a666-4680b5e37683",
   "metadata": {},
   "source": [
    "### Create a repeatable function to do this\n",
    "This should be batch-able for mapping over the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1c4c38e8-030a-419a-8ec5-998b245b8a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all text\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # Drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i:i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7419dfe7-3b34-4a23-8dc2-f64d8c7cb029",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "08ab5d16-706f-437b-9997-12dba30d940c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 61291\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 59904\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 122957\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now have more examples because of chunking\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "356327df-8a5c-45c6-8f32-73d9060cafc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arguably their answer to good old boy john ford, had sex scenes in his films. < br / > < br / > i do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in america. i am curious - yellow is a good film for anyone wanting to study the meat and potatoes ( no pun intended ) of swedish cinema. but really, this film doesn\\'t have much of a plot. [SEP] [CLS] \" i am curious : yellow \" is a risible and pretentious steaming pile. it doesn'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][2][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be83cd8-c4b6-4025-97ee-8fcf919c0620",
   "metadata": {},
   "source": [
    "## Data collation\n",
    "Need to set up random masking of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b8fb3e66-f93d-402d-97db-c815cd16f453",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c9e599-2fa3-47da-9e3c-61ea54aeee93",
   "metadata": {},
   "source": [
    "How is masking applied to our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "63b4a521-79a7-453a-bbbb-ab57e63cba8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> . because you got to [MASK] [MASK] as real characters this makes you like them more as an audience, and makes you more sympathetic to them as totally [MASK] victims of the [MASK] government, who you can not sympathise with [MASK] the singing of the students is correct because we [MASK] from accounts that the students in the riot [MASK] singing and dancing before it became violent. the clothing of the students in [MASK] [MASK] [MASK] is very similar to [MASK] clothing shown in photos [MASK] soweto [MASK] they made the movie actually [MASK] soweto, which is why it looks [MASK] accurate in many parts. all these things make the film [MASK] [MASK] for someone using\n",
      ">>> ['.', 'because', 'you', 'got', 'to', '[MASK]', '[MASK]', 'as', 'real', 'characters', 'this', 'makes', 'you', 'like', 'them', 'more', 'as', 'an', 'audience', ',', 'and', 'makes', 'you', 'more', 'sympathetic', 'to', 'them', 'as', 'totally', '[MASK]', 'victims', 'of', 'the', '[MASK]', 'government', ',', 'who', 'you', 'can', 'not', 'sy', '##mp', '##athi', '##se', 'with', '[MASK]', 'the', 'singing', 'of', 'the', 'students', 'is', 'correct', 'because', 'we', '[MASK]', 'from', 'accounts', 'that', 'the', 'students', 'in', 'the', 'riot', '[MASK]', 'singing', 'and', 'dancing', 'before', 'it', 'became', 'violent', '.', 'the', 'clothing', 'of', 'the', 'students', 'in', '[MASK]', '[MASK]', '[MASK]', 'is', 'very', 'similar', 'to', '[MASK]', 'clothing', 'shown', 'in', 'photos', '[MASK]', 'so', '##we', '##to', '[MASK]', 'they', 'made', 'the', 'movie', 'actually', '[MASK]', 'so', '##we', '##to', ',', 'which', 'is', 'why', 'it', 'looks', '[MASK]', 'accurate', 'in', 'many', 'parts', '.', 'all', 'these', 'things', 'make', 'the', 'film', '[MASK]', '[MASK]', 'for', 'someone', 'using']\n",
      ">>> with his [MASK] and [MASK] to become [MASK] rich and famous writer one day because of [MASK] observations. his family includes a mother, [MASK] father [MASK] a perfect sister, and a genius - little - brother. the first [MASK], which is going to sound a bit stupid since john scott shepard has created this situation - [MASK] the sister and mother gets pregnant cfa that's the first [MASK] hearing writer hits. then the father [MASK]s his job [MASK] the law firm. [MASK] [MASK] [MASK] gets a [MASK] attack. the middle child gets in a fight with the sister [MASK] [MASK] boyfriend. this [MASK] all in a day's work [MASK] < [MASK] / > [MASK]\n",
      ">>> ['with', 'his', '[MASK]', 'and', '[MASK]', 'to', 'become', '[MASK]', 'rich', 'and', 'famous', 'writer', 'one', 'day', 'because', 'of', '[MASK]', 'observations', '.', 'his', 'family', 'includes', 'a', 'mother', ',', '[MASK]', 'father', '[MASK]', 'a', 'perfect', 'sister', ',', 'and', 'a', 'genius', '-', 'little', '-', 'brother', '.', 'the', 'first', '[MASK]', ',', 'which', 'is', 'going', 'to', 'sound', 'a', 'bit', 'stupid', 'since', 'john', 'scott', 'shepard', 'has', 'created', 'this', 'situation', '-', '[MASK]', 'the', 'sister', 'and', 'mother', 'gets', 'pregnant', 'cfa', 'that', \"'\", 's', 'the', 'first', '[MASK]', 'hearing', 'writer', 'hits', '.', 'then', 'the', 'father', '[MASK]', '##s', 'his', 'job', '[MASK]', 'the', 'law', 'firm', '.', '[MASK]', '[MASK]', '[MASK]', 'gets', 'a', '[MASK]', 'attack', '.', 'the', 'middle', 'child', 'gets', 'in', 'a', 'fight', 'with', 'the', 'sister', '[MASK]', '[MASK]', 'boyfriend', '.', 'this', '[MASK]', 'all', 'in', 'a', 'day', \"'\", 's', 'work', '[MASK]', '<', '[MASK]', '/', '>', '[MASK]']\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"].shuffle(seed=42)[i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\">>> {tokenizer.decode(chunk)}\")\n",
    "    # We are applying masks at the token level, not the word level\n",
    "    # this means that sometimes only parts of words are masked\n",
    "    print(f\">>> {tokenizer.convert_ids_to_tokens(chunk)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1ee9b9-0a20-4204-b394-cfcf256bf95d",
   "metadata": {},
   "source": [
    "### Custom data collation for whole-word masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a873af62-e220-4b03-9bb1-78c4b8fba684",
   "metadata": {},
   "outputs": [],
   "source": [
    "wwm_probability = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7c59e675-6607-4297-a2b0-6f5051068644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_word_masking_data_collator(features, debug=False):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # Create a map between words and corresp. token indices\n",
    "        mapping = defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                # On a new word, not continuing a previous word\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        \n",
    "        # Set labels to -100 for all tokens not belonging to masked words\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        if debug:\n",
    "            feature[\"full_labels\"] = labels\n",
    "        feature[\"labels\"] = new_labels\n",
    "    \n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf1db1a-e52e-4831-ab14-08590d85de4a",
   "metadata": {},
   "source": [
    "Explore some samples using this collation approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c3b49602-faa2-4007-acaf-8d88eeef948e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> . because you got [MASK] see [MASK] as [MASK] [MASK] this makes you like them more [MASK] an audience [MASK] and makes you more [MASK] to [MASK] as totally the victims of the white [MASK], who you [MASK] not sympathise with. the [MASK] [MASK] the students is [MASK] because we know from [MASK] that the students [MASK] the riot [MASK] singing and [MASK] before [MASK] became [MASK]. [MASK] clothing of [MASK] students [MASK] sarafina is [MASK] similar to [MASK] clothing shown in [MASK] from [MASK] [MASK] [MASK]. they made the movie [MASK] in soweto, which is why it [MASK] very accurate in many parts. all [MASK] things make the [MASK] more accurate for someone using\n",
      ">>> ['.', 'because', 'you', 'got', '[MASK]', 'see', '[MASK]', 'as', '[MASK]', '[MASK]', 'this', 'makes', 'you', 'like', 'them', 'more', '[MASK]', 'an', 'audience', '[MASK]', 'and', 'makes', 'you', 'more', '[MASK]', 'to', '[MASK]', 'as', 'totally', 'the', 'victims', 'of', 'the', 'white', '[MASK]', ',', 'who', 'you', '[MASK]', 'not', 'sy', '##mp', '##athi', '##se', 'with', '.', 'the', '[MASK]', '[MASK]', 'the', 'students', 'is', '[MASK]', 'because', 'we', 'know', 'from', '[MASK]', 'that', 'the', 'students', '[MASK]', 'the', 'riot', '[MASK]', 'singing', 'and', '[MASK]', 'before', '[MASK]', 'became', '[MASK]', '.', '[MASK]', 'clothing', 'of', '[MASK]', 'students', '[MASK]', 'sara', '##fin', '##a', 'is', '[MASK]', 'similar', 'to', '[MASK]', 'clothing', 'shown', 'in', '[MASK]', 'from', '[MASK]', '[MASK]', '[MASK]', '.', 'they', 'made', 'the', 'movie', '[MASK]', 'in', 'so', '##we', '##to', ',', 'which', 'is', 'why', 'it', '[MASK]', 'very', 'accurate', 'in', 'many', 'parts', '.', 'all', '[MASK]', 'things', 'make', 'the', '[MASK]', 'more', 'accurate', 'for', 'someone', 'using']\n",
      ">>> ['.', 'because', 'you', 'got', 'to', 'see', 'them', 'as', 'real', 'characters', 'this', 'makes', 'you', 'like', 'them', 'more', 'as', 'an', 'audience', ',', 'and', 'makes', 'you', 'more', 'sympathetic', 'to', 'them', 'as', 'totally', 'the', 'victims', 'of', 'the', 'white', 'government', ',', 'who', 'you', 'can', 'not', 'sy', '##mp', '##athi', '##se', 'with', '.', 'the', 'singing', 'of', 'the', 'students', 'is', 'correct', 'because', 'we', 'know', 'from', 'accounts', 'that', 'the', 'students', 'in', 'the', 'riot', 'were', 'singing', 'and', 'dancing', 'before', 'it', 'became', 'violent', '.', 'the', 'clothing', 'of', 'the', 'students', 'in', 'sara', '##fin', '##a', 'is', 'very', 'similar', 'to', 'the', 'clothing', 'shown', 'in', 'photos', 'from', 'so', '##we', '##to', '.', 'they', 'made', 'the', 'movie', 'actually', 'in', 'so', '##we', '##to', ',', 'which', 'is', 'why', 'it', 'looks', 'very', 'accurate', 'in', 'many', 'parts', '.', 'all', 'these', 'things', 'make', 'the', 'film', 'more', 'accurate', 'for', 'someone', 'using']\n",
      ">>> with [MASK] [MASK] and hopes to become a rich and [MASK] writer one day because of his observations. his family includes a [MASK], a father, a perfect sister [MASK] and a genius - [MASK] - brother [MASK] the first [MASK], which is going to sound a bit stupid [MASK] john scott shepard [MASK] created this situation - both [MASK] [MASK] and [MASK] gets pregnant. [MASK]'s [MASK] first situation [MASK] writer [MASK]. then the [MASK] quits his job [MASK] the law firm. [MASK] youngest son gets a panic attack [MASK] the middle child gets [MASK] a fight with the [MASK]'s [MASK]. this [MASK] all [MASK] a [MASK]'s work [MASK] < br / > <\n",
      ">>> ['with', '[MASK]', '[MASK]', 'and', 'hopes', 'to', 'become', 'a', 'rich', 'and', '[MASK]', 'writer', 'one', 'day', 'because', 'of', 'his', 'observations', '.', 'his', 'family', 'includes', 'a', '[MASK]', ',', 'a', 'father', ',', 'a', 'perfect', 'sister', '[MASK]', 'and', 'a', 'genius', '-', '[MASK]', '-', 'brother', '[MASK]', 'the', 'first', '[MASK]', ',', 'which', 'is', 'going', 'to', 'sound', 'a', 'bit', 'stupid', '[MASK]', 'john', 'scott', 'shepard', '[MASK]', 'created', 'this', 'situation', '-', 'both', '[MASK]', '[MASK]', 'and', '[MASK]', 'gets', 'pregnant', '.', '[MASK]', \"'\", 's', '[MASK]', 'first', 'situation', '[MASK]', 'writer', '[MASK]', '.', 'then', 'the', '[MASK]', 'quit', '##s', 'his', 'job', '[MASK]', 'the', 'law', 'firm', '.', '[MASK]', 'youngest', 'son', 'gets', 'a', 'panic', 'attack', '[MASK]', 'the', 'middle', 'child', 'gets', '[MASK]', 'a', 'fight', 'with', 'the', '[MASK]', \"'\", 's', '[MASK]', '.', 'this', '[MASK]', 'all', '[MASK]', 'a', '[MASK]', \"'\", 's', 'work', '[MASK]', '<', 'br', '/', '>', '<']\n",
      ">>> ['with', 'his', 'family', 'and', 'hopes', 'to', 'become', 'a', 'rich', 'and', 'famous', 'writer', 'one', 'day', 'because', 'of', 'his', 'observations', '.', 'his', 'family', 'includes', 'a', 'mother', ',', 'a', 'father', ',', 'a', 'perfect', 'sister', ',', 'and', 'a', 'genius', '-', 'little', '-', 'brother', '.', 'the', 'first', 'episode', ',', 'which', 'is', 'going', 'to', 'sound', 'a', 'bit', 'stupid', 'since', 'john', 'scott', 'shepard', 'has', 'created', 'this', 'situation', '-', 'both', 'the', 'sister', 'and', 'mother', 'gets', 'pregnant', '.', 'that', \"'\", 's', 'the', 'first', 'situation', 'the', 'writer', 'hits', '.', 'then', 'the', 'father', 'quit', '##s', 'his', 'job', 'at', 'the', 'law', 'firm', '.', 'the', 'youngest', 'son', 'gets', 'a', 'panic', 'attack', '.', 'the', 'middle', 'child', 'gets', 'in', 'a', 'fight', 'with', 'the', 'sister', \"'\", 's', 'boyfriend', '.', 'this', 'is', 'all', 'in', 'a', 'day', \"'\", 's', 'work', '.', '<', 'br', '/', '>', '<']\n"
     ]
    }
   ],
   "source": [
    "# Now, if a word is masked and is split up into multiple\n",
    "# tokens, they are all masked together\n",
    "samples = [lm_datasets[\"train\"].shuffle(seed=42)[i] for i in range(2)]\n",
    "batch = whole_word_masking_data_collator(samples, debug=True)\n",
    "\n",
    "for i in range(len(batch[\"input_ids\"])):\n",
    "    print(f\">>> {tokenizer.decode(batch['input_ids'][i])}\")\n",
    "    print(f\">>> {tokenizer.convert_ids_to_tokens(batch['input_ids'][i])}\")\n",
    "    print(f\">>> {tokenizer.convert_ids_to_tokens(batch['full_labels'][i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184179e2-00b4-40ca-a9b3-939b83ea56e1",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e8e516-1f20-44f2-8013-5ace14d28890",
   "metadata": {},
   "source": [
    "## Downsample the data to avoid long runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4fdb1ad8-b31c-48aa-86d2-bec76454e3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 10_000\n",
    "test_size = int(0.1 * train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "69213e9d-aa4a-4147-80d3-1a01efff0e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsamp_dataset = lm_datasets[\"train\"].train_test_split(\n",
    "    train_size=train_size,\n",
    "    test_size=test_size,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "53ce17da-062b-4aea-9726-9ab2e4eab431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downsamp_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8febbd-8822-4efe-94f3-bc9613ec3d10",
   "metadata": {},
   "source": [
    "## Set up trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6a402f25-dfcc-453d-b8ca-41ee2f83f2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "lr = 2e-5\n",
    "weight_decay = 1e-2\n",
    "logging_steps = len(downsamp_dataset[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed821564-6568-4562-b114-0cd1830d1caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since model doesn't use word_ids, need to make sure we don't delete these\n",
    "# by setting remove_unused_columns to False\n",
    "# If don't do this, can't use `whole_word_masking_data_collator` since\n",
    "# it depends on the word_ids column being present\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"../temp/07/{model_name}-finetuned-imdb\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=lr,\n",
    "    weight_decay=weight_decay,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=False,\n",
    "    fp16=True,  # Mixed-precision training for speed boost\n",
    "    logging_steps=logging_steps,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsamp_dataset[\"train\"],\n",
    "    eval_dataset=downsamp_dataset[\"test\"],\n",
    "    data_collator=whole_word_masking_data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21517bc7-20a5-4678-9f19-5ea1b1eb93f0",
   "metadata": {},
   "source": [
    "## Train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a3a0f3-6538-430e-a85f-6763153b63b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ad615-65cb-483a-9d6e-837a211e27ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\">>> Perplexity: {np.exp(eval_results['eval_loss']).round(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebf834f-c6dd-4f9e-956f-d92e47f80dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432cfdca-b8da-4c01-94e6-8a2a99eb6423",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539e6ac6-f170-4f47-a147-c3efee4f7f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\">>> Perplexity: {np.exp(eval_results['eval_loss']).round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bab347d-a833-4eef-ae06-30db50fa71c8",
   "metadata": {},
   "source": [
    "## Train and evaluate model with Accelerate & custom loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a39c23-433f-475a-80d1-8fe1653cf030",
   "metadata": {},
   "source": [
    "### Apply masking once to validation set\n",
    "This allows for comparability of values across evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "707d1a29-0bbd-441b-ba99-697675cc6180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_random_mask(batch, collator):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = collator(features)\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d94f4806-87a6-44df-a038-682288ad011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsamp_dataset_2 = downsamp_dataset.remove_columns([\"word_ids\"])\n",
    "eval_dataset = downsamp_dataset[\"test\"].map(\n",
    "    lambda b: insert_random_mask(b, whole_word_masking_data_collator),\n",
    "    batched=True,\n",
    "    remove_columns=downsamp_dataset[\"test\"].column_names\n",
    ")\n",
    "eval_dataset = eval_dataset.rename_columns({\n",
    "    \"masked_input_ids\": \"input_ids\",\n",
    "    \"masked_attention_mask\": \"attention_mask\",\n",
    "    \"masked_labels\": \"labels\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a3e58a35-61e4-4d9c-8e6f-72775d4bbd91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18214e8-c0f0-49ae-8dde-b03c36608123",
   "metadata": {},
   "source": [
    "### Data loaders and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d2dbfcd9-a01d-4b96-a8b7-36d51892725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-5\n",
    "num_train_epochs = 3\n",
    "# batch_size = 64 won't fit in GPU memory on my machine\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9396e826-8fde-45e8-9993-7bc0b5dfe711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use default data collator\n",
    "train_dataloader = DataLoader(\n",
    "    downsamp_dataset[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=whole_word_masking_data_collator\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1f552eb4-f619-453c-a823-056ad03c2a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d20eaeed-c8c9-4b2f-b307-1e6504c1f150",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "eeea4395-1734-45ac-813c-a197c8937580",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model,\n",
    "    optimizer,\n",
    "    train_dataloader,\n",
    "    eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cb1acb7f-d4bb-4f6d-a691-5ccaf105afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bde42969-7085-4667-bbca-dc96b17990e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1872190-5ac5-4487-9966-c39986910917",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4a705a0a-b7eb-406a-b2f2-e0c22e5d7ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf16502d0de5474b826c00de0e92022d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/939 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 0, Perplexity: 25.044612884521484\n",
      ">>> Epoch 1, Perplexity: 24.35580062866211\n",
      ">>> Epoch 2, Perplexity: 24.35580062866211\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "output_dir = \"../temp/07/distilbert-base-uncased-finetuned-imdb-accelerate\"\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    torch.cuda.empty_cache()\n",
    "    # Training block\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation block\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[:len(eval_dataset)]\n",
    "    try:\n",
    "        perplexity = torch.exp(torch.mean(losses)).item()\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    print(f\">>> Epoch {epoch}, Perplexity: {perplexity}\")\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(\n",
    "        output_dir,\n",
    "        save_function=accelerator.save\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba082dd-8b5e-43cf-afb8-3c8959b1e3e8",
   "metadata": {},
   "source": [
    "## Use the fine-tuned model in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "685fbc82-044b-46e7-88a2-9084fa9c5219",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_filler = pipeline(\n",
    "    \"fill-mask\", model=output_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b70a28bc-56d3-4ee0-a32f-107925a6d973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> this is a great film.\n",
      ">>> this is a great movie.\n",
      ">>> this is a great idea.\n",
      ">>> this is a great one.\n",
      ">>> this is a great comedy.\n"
     ]
    }
   ],
   "source": [
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645ee908-4694-4484-b1c5-3dca2b99bb97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
