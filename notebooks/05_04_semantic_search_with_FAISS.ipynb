{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2aa09109-720b-4b49-b33a-147e4457637a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0773e080-bf22-46d1-ab78-f17758251604",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = multiprocessing.cpu_count()\n",
    "num_cores_avail = max(1, num_cores - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285b84fc-7e2d-484d-9a89-acc08689e2cd",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "620198e7-b044-4f5b-9caa-844150c17366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# Just get the training split\n",
    "issues_dataset = load_dataset(\"lewtun/github-issues\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d251d27d-1155-4b7c-8eb4-60af28b185e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 3019\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c55122-a3ce-4567-9cef-8c3e2001a2cb",
   "metadata": {},
   "source": [
    "# Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03a1f47a-f896-4140-b65b-c506cde7e78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_dataset = issues_dataset.filter(\n",
    "    lambda x: (x[\"is_pull_request\"] == False and len(x[\"comments\"]) > 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "509f8d7b-2e7f-4d53-b9a7-6365e82184ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f453da1a-ebce-43c8-bef5-9e151fef6a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = issues_dataset.column_names\n",
    "keeper_cols = [\"title\", \"body\", \"html_url\", \"comments\"]\n",
    "drop_cols = set(keeper_cols).symmetric_difference(cols)\n",
    "issues_dataset = issues_dataset.remove_columns(drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "704c9e03-e5ee-478d-a97e-bc8cbcc2985f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58442cae-5e6e-49b8-afc6-736f3521f983",
   "metadata": {},
   "source": [
    "## 'Explode' data to have 1 row per comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d0bdc2a-c828-4bb8-9ec6-7d046a20757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_dataset.set_format(\"pandas\")\n",
    "df = issues_dataset[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14cbf58f-fbf3-4a3f-b200-513317f472be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cool, I think we can do both :)',\n",
       " '@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"comments\"][0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a6b30ae-87d1-41ef-b653-dc685caff78b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>Cool, I think we can do both :)</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>@lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>Hi ! I guess the caching mechanism should have...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>If it's easy enough to implement, then yes ple...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            html_url  \\\n",
       "0  https://github.com/huggingface/datasets/issues...   \n",
       "1  https://github.com/huggingface/datasets/issues...   \n",
       "2  https://github.com/huggingface/datasets/issues...   \n",
       "3  https://github.com/huggingface/datasets/issues...   \n",
       "\n",
       "                                               title  \\\n",
       "0                              Protect master branch   \n",
       "1                              Protect master branch   \n",
       "2  Backwards compatibility broken for cached data...   \n",
       "3  Backwards compatibility broken for cached data...   \n",
       "\n",
       "                                            comments  \\\n",
       "0                    Cool, I think we can do both :)   \n",
       "1  @lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...   \n",
       "2  Hi ! I guess the caching mechanism should have...   \n",
       "3  If it's easy enough to implement, then yes ple...   \n",
       "\n",
       "                                                body  \n",
       "0  After accidental merge commit (91c55355b634d0d...  \n",
       "1  After accidental merge commit (91c55355b634d0d...  \n",
       "2  ## Describe the bug\\r\\nAfter upgrading to data...  \n",
       "3  ## Describe the bug\\r\\nAfter upgrading to data...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df = df.explode(\"comments\", ignore_index=True)\n",
    "comments_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29bd8d4e-0718-44df-9e31-850461da114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_dataset = Dataset.from_pandas(comments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99e4a79c-9c4c-4298-8364-580d5b5149e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 2964\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fb9330-3160-4109-8ceb-7cab1a600929",
   "metadata": {},
   "source": [
    "## Try to do this without pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c98e550-a512-479e-b35f-28ec9a163842",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_dataset.reset_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b76620f-7954-4086-ab8f-daabc6ef6dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_explode(examples):\n",
    "    exploded_data = {\"html_url\": [], \"title\": [], \"comment\": [], \"body\": []}\n",
    "    for html_url, title, comments, body in zip(examples[\"html_url\"], examples[\"title\"], examples[\"comments\"], examples[\"body\"]):\n",
    "        for comment in comments: \n",
    "            exploded_data[\"html_url\"].append(html_url)\n",
    "            exploded_data[\"title\"].append(title)\n",
    "            exploded_data[\"comment\"].append(comment)\n",
    "            exploded_data[\"body\"].append(body)\n",
    "    return exploded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b283035b-b12e-44da-972a-4beb4b47a2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5184537c08014f26a26bc0caf547fae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/808 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comments_dataset = issues_dataset.map(batched_explode, batched=True, remove_columns=[\"html_url\", \"title\", \"comments\", \"body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96f5d014-ab42-488e-8f7d-20611bf06be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'body', 'comment'],\n",
       "    num_rows: 2964\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aea1846b-e4cc-4211-be5f-43bec1beef7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "      <td>Cool, I think we can do both :)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "      <td>@lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "      <td>Hi ! I guess the caching mechanism should have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "      <td>If it's easy enough to implement, then yes ple...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            html_url  \\\n",
       "0  https://github.com/huggingface/datasets/issues...   \n",
       "1  https://github.com/huggingface/datasets/issues...   \n",
       "2  https://github.com/huggingface/datasets/issues...   \n",
       "3  https://github.com/huggingface/datasets/issues...   \n",
       "\n",
       "                                               title  \\\n",
       "0                              Protect master branch   \n",
       "1                              Protect master branch   \n",
       "2  Backwards compatibility broken for cached data...   \n",
       "3  Backwards compatibility broken for cached data...   \n",
       "\n",
       "                                                body  \\\n",
       "0  After accidental merge commit (91c55355b634d0d...   \n",
       "1  After accidental merge commit (91c55355b634d0d...   \n",
       "2  ## Describe the bug\\r\\nAfter upgrading to data...   \n",
       "3  ## Describe the bug\\r\\nAfter upgrading to data...   \n",
       "\n",
       "                                             comment  \n",
       "0                    Cool, I think we can do both :)  \n",
       "1  @lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...  \n",
       "2  Hi ! I guess the caching mechanism should have...  \n",
       "3  If it's easy enough to implement, then yes ple...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset.set_format(\"pandas\")\n",
    "comments_dataset[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0def27f-15ff-4d26-9466-fe304c316fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_dataset.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f09b30-0706-4d7f-afa3-a37da04f18bf",
   "metadata": {},
   "source": [
    "## Filter out short comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69165791-3916-45c7-bb9b-3fa0826d789a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9472e22bfd3c4a3fbd2d90f2c6960243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.map(lambda x: {\"comment_length\": [len(c.split(' ')) for c in x[\"comment\"]]}, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba349427-0be2-4550-9cdd-4c462ce334b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'body', 'comment', 'comment_length'],\n",
       "    num_rows: 2964\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "593c64ee-674d-4c48-95d2-f391746791ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'html_url': 'https://github.com/huggingface/datasets/issues/2945',\n",
       " 'title': 'Protect master branch',\n",
       " 'body': 'After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - Currently, simple merge commits are already disabled\\r\\n  - I propose to disable rebase merging as well\\r\\n- ~~Protect the master branch from direct pushes (to avoid accidentally pushing of merge commits)~~\\r\\n  - ~~This protection would reject direct pushes to master branch~~\\r\\n  - ~~If so, for each release (when we need to commit directly to the master branch), we should previously disable the protection and re-enable it again after the release~~\\r\\n- [x] Protect the master branch only from direct pushing of **merge commits**\\r\\n  - GitHub offers the possibility to protect the master branch only from merge commits (which are the ones that introduce all the commits from the feature branch into the master branch).\\r\\n  - No need to disable/re-enable this protection on each release \\r\\n\\r\\nThis purpose of this Issue is to open a discussion about this problem and to agree in a solution.',\n",
       " 'comment': 'Cool, I think we can do both :)',\n",
       " 'comment_length': 8}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "996e09bf-b520-4dc0-85e6-172a468ecdc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e1fd8ee36d441a805485ce8d110e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.filter(lambda x: x[\"comment_length\"] > 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf64ec50-b318-4518-a059-d5da727b2e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'body', 'comment', 'comment_length'],\n",
       "    num_rows: 2162\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac06fb3c-af13-4ec2-a30a-9d36852f8fd2",
   "metadata": {},
   "source": [
    "## Add a new column with all text concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6612d0d-ff70-4b81-923c-c956c127155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_text(example):\n",
    "    return {\"text\": f\"{example['title']} \\n {example['body']} \\n {example['comment']}\"}\n",
    "\n",
    "def batched_concat_text(examples):\n",
    "    return {\"text\": [\n",
    "        f\"{title} \\n {body} \\n {comment}\" for title, comment, body in zip(examples[\"title\"], examples[\"comment\"], examples[\"body\"])\n",
    "    ]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0584aba7-a302-4349-a310-6df0fa2041b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b71265eb24c4e6095c3037cba3505e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2162 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'body', 'comment', 'comment_length', 'text'],\n",
       "    num_rows: 2162\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset.map(concat_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc32c75f-cdf6-4de7-bbb7-533ac1c891a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc4d0befc7f4ca3b9d52e0d70cb6bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2162 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.map(batched_concat_text, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b35ba43b-ff60-4e72-a0a4-49684198208e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'body', 'comment', 'comment_length', 'text'],\n",
       "    num_rows: 2162\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd28e46-e990-4397-9587-fb7e4a705abe",
   "metadata": {},
   "source": [
    "# Text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d96f96b-58f2-428b-a133-b543886ad6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9724ae3b-bdb8-40c7-8a51-f3e6c89fed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7fc27770-4023-4a21-8873-cd4490dbfb46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetModel(\n",
       "  (embeddings): MPNetEmbeddings(\n",
       "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): MPNetEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (pooler): MPNetPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6803b3d-cf08-4ad1-9854-db4c068ff75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]\n",
    "\n",
    "\n",
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b26bbe1-d813-4664-92e7-7547208f76a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Protect master branch \\n After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - Currently, simple merge commits are already disabled\\r\\n  - I propose to disable rebase merging as well\\r\\n- ~~Protect the master branch from direct pushes (to avoid accidentally pushing of merge commits)~~\\r\\n  - ~~This protection would reject direct pushes to master branch~~\\r\\n  - ~~If so, for each release (when we need to commit directly to the master branch), we should previously disable the protection and re-enable it again after the release~~\\r\\n- [x] Protect the master branch only from direct pushing of **merge commits**\\r\\n  - GitHub offers the possibility to protect the master branch only from merge commits (which are the ones that introduce all the commits from the feature branch into the master branch).\\r\\n  - No need to disable/re-enable this protection on each release \\r\\n\\r\\nThis purpose of this Issue is to open a discussion about this problem and to agree in a solution. \\n @lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "272fade2-844a-4618-925f-7d24260eee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Quick test run\n",
    "    embedding = get_embeddings(comments_dataset[\"text\"][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7724a01-c6e3-4185-b750-6833d7ba4973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "97d5f1ff-27b5-4872-8fd6-824b8a57b787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up GPU memory\n",
    "del embedding\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee975a5-47d6-4e22-9f1c-dd9db85bb210",
   "metadata": {},
   "source": [
    "## Add embeddings to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428ec53a-d059-4960-8143-d5c05afe8000",
   "metadata": {},
   "source": [
    "### One at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e14366e-66ab-43bc-b49c-1e777ba1da15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     embeddings_dataset = comments_dataset.map(\n",
    "#         lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).detach().cpu().numpy().squeeze()}\n",
    "#     )\n",
    "#     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5d9518-1d71-4738-9149-66780dba367e",
   "metadata": {},
   "source": [
    "### Batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4582f72d-c320-4513-a347-a3dd60148d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0dfe6fd008d44efb4d2a421c8689507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2162 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def batched_get_embeddings_wrapper(examples):\n",
    "    embeddings = [row for row in get_embeddings(examples[\"text\"]).detach().cpu().numpy()]\n",
    "    # Don't let GPU memory overload \n",
    "    torch.cuda.empty_cache()\n",
    "    return {\"embeddings\": embeddings}\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings_dataset = comments_dataset.map(\n",
    "        # lambda x: {\"embeddings\": [row for row in get_embeddings(x[\"text\"]).detach().cpu().numpy()]},\n",
    "        batched_get_embeddings_wrapper,\n",
    "        batched=True,\n",
    "        batch_size=64,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270e798c-8b57-4006-b4ce-39b6c805f62a",
   "metadata": {},
   "source": [
    "# FAISS similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c65e91b-226d-4af4-a098-2bcfb27560bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc1568acf514030a04d267abfb30d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'body', 'comment', 'comment_length', 'text', 'embeddings'],\n",
       "    num_rows: 2162\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faiss_idx = \"embeddings\"\n",
    "embeddings_dataset.add_faiss_index(column=faiss_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40eb459-b5d2-432d-a8c6-ed7c4d96c267",
   "metadata": {},
   "source": [
    "## Run a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ea8faad5-ec89-4810-9b19-34f4682d9dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How can I load a dataset offline?\"\n",
    "with torch.no_grad():\n",
    "    question_embedding = get_embeddings([question]).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5454f98f-c86f-4a30-9ac1-390ffd9410eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c7af4840-8ddb-4d01-865b-630cb1a8acad",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "scores, samples = embeddings_dataset.get_nearest_examples(\n",
    "    faiss_idx,\n",
    "    question_embedding,\n",
    "    k=k\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c7102573-689a-4941-a0ec-6f7ea1f83610",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df = (\n",
    "    pd.DataFrame.from_dict(samples)\n",
    "        .assign(score=scores)\n",
    "        .sort_values(by=\"score\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ecb81947-b9f5-45b7-80f4-00f3e2e7fc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** SCORE: 25.505 ****\n",
      "\n",
      "COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.\n",
      "\n",
      "@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "**** SCORE: 24.555 ****\n",
      "\n",
      "COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)\n",
      "You can now use them offline\n",
      "```python\n",
      "datasets = load_dataset('text', data_files=data_files)\n",
      "```\n",
      "\n",
      "We'll do a new release soon\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "**** SCORE: 24.149 ****\n",
      "\n",
      "COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.\n",
      "\n",
      "Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :) \n",
      "\n",
      "I already note the \"freeze\" modules option, to prevent local modules updates. It would be a cool feature.\n",
      "\n",
      "----------\n",
      "\n",
      "> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n",
      "\n",
      "Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.\n",
      "For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do\n",
      "```python\n",
      "load_dataset(\"./my_dataset\")\n",
      "```\n",
      "and the dataset script will generate your dataset once and for all.\n",
      "\n",
      "----------\n",
      "\n",
      "About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.\n",
      "cf #1724 \n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "**** SCORE: 22.894 ****\n",
      "\n",
      "COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine\n",
      "> \n",
      "> 1. (online machine)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> import datasets\n",
      "> \n",
      "> data = datasets.load_dataset(...)\n",
      "> \n",
      "> data.save_to_disk(/YOUR/DATASET/DIR)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> 2. copy the dir from online to the offline machine\n",
      "> \n",
      "> 3. (offline machine)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> import datasets\n",
      "> \n",
      "> data = datasets.load_from_disk(/SAVED/DATA/DIR)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> \n",
      "> \n",
      "> HTH.\n",
      "\n",
      "\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "**** SCORE: 22.407 ****\n",
      "\n",
      "COMMENT: here is my way to load a dataset offline, but it **requires** an online machine\n",
      "1. (online machine)\n",
      "```\n",
      "import datasets\n",
      "data = datasets.load_dataset(...)\n",
      "data.save_to_disk(/YOUR/DATASET/DIR)\n",
      "```\n",
      "2. copy the dir from online to the offline machine\n",
      "3. (offline machine)\n",
      "```\n",
      "import datasets\n",
      "data = datasets.load_from_disk(/SAVED/DATA/DIR)\n",
      "```\n",
      "\n",
      "HTH.\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _, row in samples_df.iterrows():\n",
    "    print(f\"{'*'*4} SCORE: {round(row.score, 3)} {'*'*4}\\n\")\n",
    "    print(f\"COMMENT: {row.comment}\")\n",
    "    print(f\"TITLE: {row.title}\")\n",
    "    print(f\"URL: {row.html_url}\")\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
