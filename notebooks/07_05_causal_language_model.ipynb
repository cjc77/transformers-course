{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c84058b-ff8a-4352-ab7f-32b193a08323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from accelerate import Accelerator\n",
    "from datasets import Dataset, load_dataset, DatasetDict, load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    AutoConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    get_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b99181bc-b1da-493e-a45b-8177a2a70f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores_avail = max(1, multiprocessing.cpu_count() - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc76db6-f6cf-4933-9712-6432e5a544d5",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47ba0e75-54dd-4989-a10e-6de39f827bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def any_keyword_in_string(string, keywords):\n",
    "    for keyword in keywords:\n",
    "        if keyword in string:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd3f5d6e-a08d-43ab-877e-0fed474c7c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
    "example_1 = \"import numpy as np\"\n",
    "example_2 = \"import pandas as pd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a655a0f-a619-41b9-ae9d-8d13670ece5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False True\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    any_keyword_in_string(example_1, filters),\n",
    "    any_keyword_in_string(example_2, filters)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9850e667-f861-458f-a012-abd75dfcde7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_streaming_dataset(dataset, filters):\n",
    "    filtered_dict = defaultdict(list)\n",
    "    total = 0\n",
    "    for sample in tqdm(iter(dataset)):\n",
    "        total += 1\n",
    "        if any_keyword_in_string(sample[\"content\"], filters):\n",
    "            for k, v in sample.items():\n",
    "                filtered_dict[k].append(v)\n",
    "    post_filt_prop = round(len(filtered_dict['content']) / total, ndigits=2)\n",
    "    print(f\"{post_filt_prop * 100}% of data after filtering\")\n",
    "    return Dataset.from_dict(filtered_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb26d36a-c46e-42fa-9c9d-6e161aab54f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_checkpoint = f\"transformersbook/codeparrot\"\n",
    "dataset_commit_id = \"0933803eb0f5956b2da9d2d7b6805fa31b18a6c8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41e8cd8d-18ff-485d-8427-099f87479a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f06e807389f4a44a88bab280b10e3e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "split = \"train\"\n",
    "data = load_dataset(f\"{dataset_checkpoint}-{split}\", revision=dataset_commit_id, split=split, streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7535f1f3-628c-47d1-8db6-e15aba6a1409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip this because it takes quite a while\n",
    "# filtered_data = filter_streaming_dataset(data, filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddaac2d0-77fc-401e-8ef4-701cf69203f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepped_dataset_checkpoint = \"huggingface-course/codeparrot-ds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "852d0161-ff99-43c8-a324-d43c4ae8b564",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = load_dataset(f\"{prepped_dataset_checkpoint}-train\", split=\"train\")\n",
    "ds_valid = load_dataset(f\"{prepped_dataset_checkpoint}-valid\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "266ff2e1-117e-42bf-b3cc-95261a41c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = DatasetDict({\n",
    "    \"train\": ds_train,\n",
    "    \"valid\": ds_valid\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ae32655-b942-4717-916a-b7166d5a005a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 606720\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 3322\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd9caf13-b76b-460d-a065-cf391d249f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_samp = int(0.0025 * raw_datasets[\"train\"].num_rows)\n",
    "n_valid_samp = int(0.1 * raw_datasets[\"valid\"].num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c4d172f-5ab4-49b8-b57a-52a7727cdee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets_mini = DatasetDict()\n",
    "raw_datasets_mini[\"train\"] = raw_datasets[\"train\"].shuffle(seed=42).select(range(n_train_samp))\n",
    "raw_datasets_mini[\"valid\"] = raw_datasets[\"valid\"].shuffle(seed=42).select(range(n_valid_samp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1460b290-b32c-4b54-a2f1-0306881a182f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 1516\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 332\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dae06caf-f334-4e3c-a04e-2b5ca68b9391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO_NAME: ThomasMiconi/htmresearch\n",
      "PATH: projects/feedback/feedback_sequences.py\n",
      "COPIES: 2\n",
      "SIZE: 26875\n",
      "CONTENT: \n",
      "# Numenta Platform for Intelligent Computing (NuPIC)\n",
      "# Copyright (C) 2016, Numenta, Inc.  Unless you have an agreement\n",
      "# with Numenta, Inc., for a separate license for this software code, the\n",
      "# follo\n",
      "LICENSE: agpl-3.0\n"
     ]
    }
   ],
   "source": [
    "for k, v in raw_datasets_mini[\"train\"][0].items():\n",
    "    print(f\"{k.upper()}: {v[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f16005-feaf-4a73-9729-dd24a5ffff9e",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "629dab08-87e0-4bc7-9796-a311b84e5831",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_checkpoint = \"huggingface-course/code-search-net-tokenizer\"\n",
    "tokenizer_commit_id = \"2a84d6753fdeb105c5e2e9a6be952f119216a991\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db7a3764-7933-48e8-af45-0dc471d1ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 128\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint, revision=tokenizer_commit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f2ab41f-8be3-444c-8c60-7e57c0a479c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tokenizer(\n",
    "    raw_datasets_mini[\"train\"][:2][\"content\"],\n",
    "    truncation=True,\n",
    "    max_length=context_length,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_length=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10554c4a-5d6b-444b-aa18-ae4f993450c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs length: 86\n",
      "Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 17, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 2]\n",
      "Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
    "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8aa5ce4-a3fa-49a3-b592-8a82176c9f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9169b75-4699-41cd-b7d6-e4a46a9dc7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_save_dir = \"../temp/07/codeparrot-ds-tokenized-dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82dfe5bc-a3e6-4519-b7c6-0b31309ee552",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(f\"{dataset_save_dir}\"):\n",
    "    tokenized_datasets = load_from_disk(dataset_save_dir)\n",
    "else:\n",
    "    tokenized_datasets = raw_datasets_mini.map(\n",
    "        tokenize, batched=True, remove_columns=raw_datasets_mini[\"train\"].column_names\n",
    "    )\n",
    "    tokenized_datasets.save_to_disk(dataset_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03143bf0-895e-4aaf-980a-4b5810a02476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 39588\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 8898\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf70d66-f506-4bf4-818e-f850276f4531",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7eab9f0-5d13-4253-84ec-552644f7cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0b66e4e-4419-42ea-9856-7236bc340a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8326f0c3-cecb-409c-b757-1e0b5d752edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 124.2M params\n"
     ]
    }
   ],
   "source": [
    "print(f\"GPT-2 size: {round(model_size / (1e6), 1)}M params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eef21504-2740-47aa-a22b-ec1a0cb29b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d36af8f8-46ae-4dbd-901c-5716aa45cc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([5, 128])\n",
      "attention_mask shape: torch.Size([5, 128])\n",
      "labels shape: torch.Size([5, 128])\n"
     ]
    }
   ],
   "source": [
    "outputs = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\n",
    "for k, v in outputs.items():\n",
    "    print(f\"{k} shape: {v.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32644e0-2358-4a02-8f99-a5eda6e30abc",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e804191-8422-47dc-b45f-beee007deed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_dir = \"../temp/07/codeparrot-ds-finetuned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00b868ed-c58b-4689-a2bf-bb4d8b9e504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=model_output_dir,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    logging_steps=50,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=50,\n",
    "    save_total_limit=3,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c92ba0d5-baba-4fa4-bae3-4ea81b723e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55565864-7cce-4e95-a711-0146b963a90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ef556c-6710-490f-8870-0ce49011a799",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f87d389-2e73-4807-9f07-4c6aeb1daf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d01842c-b983-4ee5-b835-49739cffb69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoints = [d for d in os.listdir(model_output_dir) if \"checkpoint\" in d]\n",
    "model_checkpoints = sorted(model_checkpoints, key=lambda x: int(x.split(\"-\")[-1]))\n",
    "latest_model_checkpoint = model_checkpoints[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "caf74c0b-7515-4971-bb59-dae1130dfc46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoint-150'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_model_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d5c139e2-ca84-4ec8-a198-f0365ca24201",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_pipeline = pipeline(\n",
    "    \"text-generation\", model=f\"{model_output_dir}/{latest_model_checkpoint}\", device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a9f458c-bc12-411d-97c7-88861349fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"\\\n",
    "# create some data\n",
    "x = np.random.randn(100)\n",
    "y = np.random.randn(100)\n",
    "\n",
    "# create scatter plot with x, y\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a35fecda-5f0a-42cb-9ba5-78860903a1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/home/carcook/dev/transformers-course/myenv/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (50) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# create some data\n",
      "x = np.random.randn(100)\n",
      "y = np.random.randn(100)\n",
      "\n",
      "# create scatter plot with x, y\n",
      "################################\t)\n",
      "\n",
      "#\n",
      "# for the X [1\n"
     ]
    }
   ],
   "source": [
    "print(gen_pipeline(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "58e4ebee-61f9-4006-b40f-39cc3bab445b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# create some data\n",
      "x = np.random.randn(100)\n",
      "y = np.random.randn(100)\n",
      "\n",
      "# create dataframe from x and y\n",
      "# -1.0.k_2..c = np\n"
     ]
    }
   ],
   "source": [
    "txt = \"\"\"\\\n",
    "# create some data\n",
    "x = np.random.randn(100)\n",
    "y = np.random.randn(100)\n",
    "\n",
    "# create dataframe from x and y\n",
    "\"\"\"\n",
    "print(gen_pipeline(txt, num_return_sequences=1)[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90837382-46a7-4d01-b437-4d7d9599b2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# dataframe with profession, income and name\n",
      "df = pd.DataFrame({'profession': x, 'income':y, 'name': z})\n",
      "\n",
      "# calculate the mean income per profession\n",
      "#\n",
      "# the test\n"
     ]
    }
   ],
   "source": [
    "txt = \"\"\"\\\n",
    "# dataframe with profession, income and name\n",
    "df = pd.DataFrame({'profession': x, 'income':y, 'name': z})\n",
    "\n",
    "# calculate the mean income per profession\n",
    "\"\"\"\n",
    "print(gen_pipeline(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bf8df5ab-cce7-4149-90c1-580f4c54873a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# import random forest regressor from scikit-learn\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "# fit random forest model with 300 estimators on X, y:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "import matplotlib.0\n",
      "\n",
      "# and the :\n"
     ]
    }
   ],
   "source": [
    "txt = \"\"\"\n",
    "# import random forest regressor from scikit-learn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# fit random forest model with 300 estimators on X, y:\n",
    "\"\"\"\n",
    "print(gen_pipeline(txt, num_return_sequences=1)[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ec3803-5833-472f-a114-0980ca1453af",
   "metadata": {},
   "source": [
    "# Using Accelerate for custom training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68d2de3c-13f0-47c9-b989-823cf017b910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword does not have single token: testtest\n"
     ]
    }
   ],
   "source": [
    "keytoken_ids = []\n",
    "for keyword in [\n",
    "    \"plt\",\n",
    "    \"pd\",\n",
    "    \"sk\",\n",
    "    \"fit\",\n",
    "    \"predict\",\n",
    "    \" plt\",\n",
    "    \" pd\",\n",
    "    \" sk\",\n",
    "    \" fit\",\n",
    "    \" predict\",\n",
    "    \"testtest\",\n",
    "]:\n",
    "    ids = tokenizer([keyword]).input_ids[0]\n",
    "    if len(ids) == 1:\n",
    "        keytoken_ids.append(ids[0])\n",
    "    else:\n",
    "        print(f\"Keyword does not have single token: {keyword}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "27db556b-2547-4296-9ca4-6767fb1035b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):\n",
    "    # Shift tokens, < n predict n\n",
    "    shift_labels = inputs[..., 1:].contiguous()\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "\n",
    "    # Calculate per-token loss\n",
    "    loss_fct = CrossEntropyLoss(reduce=False)\n",
    "    loss = loss_fct(\n",
    "        shift_logits.view(-1, shift_logits.size(-1)),\n",
    "        shift_labels.view(-1)\n",
    "    )\n",
    "    # Resize and average loss per sample\n",
    "    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)\n",
    "    # Calculate and scale weighting\n",
    "    weights = torch.stack([\n",
    "        (inputs == kt).float() for kt in keytoken_ids\n",
    "    ]).sum(axis=[0, 2])\n",
    "    weights = alpha * (1.0 + weights)\n",
    "    # Calculate weighted average\n",
    "    weighted_loss = (loss_per_sample * weights).mean()\n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fdac8d56-7bbb-4b3c-91f0-367a5e505876",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets.set_format(\"torch\")\n",
    "# train_dataloader = DataLoader(tokenized_datasets[\"train\"].select(range(100)), batch_size=32, shuffle=True)\n",
    "# valid_dataloader = DataLoader(tokenized_datasets[\"valid\"].select(range(100)), batch_size=32)\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=32, shuffle=True)\n",
    "valid_dataloader = DataLoader(tokenized_datasets[\"valid\"], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0abf48f6-4b3f-4c73-ba6b-493e58a9a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 0.1\n",
    "\n",
    "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [\n",
    "        {\"params\": params_with_wd, \"weight_decay\": weight_decay},\n",
    "        {\"params\": params_without_wd, \"weight_decay\": 0.0}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bbe3bb04-3e10-4df5-a127-71cb95d656ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(valid_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "            if outputs.loss.dim() == 0:\n",
    "                outputs.loss = outputs.loss.unsqueeze(0)\n",
    "\n",
    "        losses.append(accelerator.gather(outputs.loss))\n",
    "    loss = torch.mean(torch.cat(losses))\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    return loss.item(), perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "141bfdf2-b2f4-4169-af8c-76ac96a89351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild model from scratch\n",
    "model = GPT2LMHeadModel(config)\n",
    "optimizer = AdamW(get_grouped_params(model), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1f284b0c-d746-4714-b06e-cd702f86c91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "model, optimizer, train_dataloader, valid_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, valid_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fc58eb73-feea-4e3e-9f3c-ce0578739e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 1\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=1_000,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3e17784c-041a-48a3-aa21-7731067e8b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accel_output_dir = \"../temp/07/codeparrot-ds-accelerate-finetuned/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aa0d2f17-213c-435c-8c48-e3317c9b50e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11.030363082885742, 61719.98828125)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ef9ae4da-a2f8-41b5-be39-1cd75c936d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32aa50d44dfc4ef38aed88e1233b4961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carcook/dev/transformers-course/myenv/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': [6e-06, 6e-06], 'steps': 12, 'loss/train': 131.64871215820312}\n",
      "{'loss/eval': 9.841143608093262, 'perplexity': 18791.193359375}\n",
      "{'lr': [1.2e-05, 1.2e-05], 'steps': 24, 'loss/train': 106.70005798339844}\n",
      "{'loss/eval': 9.283154487609863, 'perplexity': 10755.3056640625}\n",
      "{'lr': [1.85e-05, 1.85e-05], 'steps': 37, 'loss/train': 126.12361145019531}\n",
      "{'loss/eval': 9.041620254516602, 'perplexity': 8447.453125}\n",
      "{'lr': [2.4500000000000003e-05, 2.4500000000000003e-05], 'steps': 49, 'loss/train': 94.83934020996094}\n",
      "{'loss/eval': 8.54878044128418, 'perplexity': 5160.45703125}\n",
      "{'lr': [3.1e-05, 3.1e-05], 'steps': 62, 'loss/train': 89.09877014160156}\n",
      "{'loss/eval': 8.021772384643555, 'perplexity': 3046.572265625}\n",
      "{'lr': [3.7e-05, 3.7e-05], 'steps': 74, 'loss/train': 113.5152359008789}\n",
      "{'loss/eval': 7.552396774291992, 'perplexity': 1905.3038330078125}\n",
      "{'lr': [4.35e-05, 4.35e-05], 'steps': 87, 'loss/train': 86.49349975585938}\n",
      "{'loss/eval': 7.156615734100342, 'perplexity': 1282.56298828125}\n",
      "{'lr': [4.9500000000000004e-05, 4.9500000000000004e-05], 'steps': 99, 'loss/train': 80.69032287597656}\n",
      "{'loss/eval': 6.7441511154174805, 'perplexity': 849.0780639648438}\n",
      "{'lr': [5.6e-05, 5.6e-05], 'steps': 112, 'loss/train': 60.2724494934082}\n",
      "{'loss/eval': 6.409576416015625, 'perplexity': 607.63623046875}\n",
      "{'lr': [6.2e-05, 6.2e-05], 'steps': 124, 'loss/train': 62.32920455932617}\n",
      "{'loss/eval': 6.14442777633667, 'perplexity': 466.11285400390625}\n",
      "{'lr': [6.850000000000001e-05, 6.850000000000001e-05], 'steps': 137, 'loss/train': 69.57198333740234}\n",
      "{'loss/eval': 5.957544326782227, 'perplexity': 386.6594543457031}\n",
      "{'lr': [7.45e-05, 7.45e-05], 'steps': 149, 'loss/train': 47.96332550048828}\n",
      "{'loss/eval': 5.814759254455566, 'perplexity': 335.210693359375}\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "gradient_accumulation_steps = 8\n",
    "eval_steps = 100\n",
    "\n",
    "model.train()\n",
    "completed_steps = 0\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    for step, batch in tqdm(\n",
    "        enumerate(train_dataloader, start=1), total=num_training_steps\n",
    "    ):\n",
    "        logits = model(batch[\"input_ids\"]).logits\n",
    "        loss = keytoken_weighted_loss(batch[\"input_ids\"], logits, keytoken_ids)\n",
    "        if step % 100 == 0:\n",
    "            accelerator.print({\n",
    "                \"lr\": lr_scheduler.get_lr(),\n",
    "                \"steps\": completed_steps,\n",
    "                \"loss/train\": loss.item() * gradient_accumulation_steps,\n",
    "            })\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "        if step % gradient_accumulation_steps == 0:\n",
    "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            completed_steps += 1\n",
    "        if (step % eval_steps * gradient_accumulation_steps) == 0:\n",
    "            eval_loss, perplexity = evaluate()\n",
    "            accelerator.print({\"loss/eval\": eval_loss, \"perplexity\": perplexity})\n",
    "            model.train()\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(model_accel_output_dir, save_function=accelerator.save)\n",
    "            if accelerator.is_main_process:\n",
    "                tokenizer.save_pretrained(model_accel_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a2de8a-53d7-4dd8-b5a5-ef6697dcae78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
