{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a22f2d6f-572d-4395-9d92-cf082b458c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "06106a7a-9025-4fc5-8765-689dd998d401",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b55efb82-1475-41d7-8635-eb14414238c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4079e6df-9242-451e-a2fe-5e6c0ba7ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49711ede-6d9c-4c89-9512-97b7e38c0c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
       "          2607,  2026,  2878,  2166,  1012,   102],\n",
       "        [  101,  2023,  2607,  2003,  6429,   999,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e504c1ac-b452-4209-bcd0-ef3170b6ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"labels\"] = torch.tensor([1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb8ad604-9739-4a5e-acc8-8444245e9d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
       "          2607,  2026,  2878,  2166,  1012,   102],\n",
       "        [  101,  2023,  2607,  2003,  6429,   999,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([1, 1])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa45099-53f4-460d-ad00-f60fe02e2de6",
   "metadata": {},
   "source": [
    "# Single-batch training using standard Pytorch approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0d71da3-ad4c-459d-8563-225dc7522535",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1f7b42-8ae6-4536-bd5f-7b6ad41394bd",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6c49b7-4442-4a0c-bd3a-169bede6cbdd",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be65dda3-1adf-453c-b235-45f91ac0ddc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0b03c7ecf442f58fd882b6a75aeedf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/28.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7a56339b0642de9a9b82ce34b995a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/28.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "483510947106450980fcf025f0a128bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/27.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691fd10c0a1d4f49bf269b6cc4b55507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15297e5253d4e8bab8443db0fe1711b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a401594716446818ece6267648f905c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e45193cff44171a938a6910789530f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bca76e6f5504eb998ad5a1ed31fad0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a79fc391d444d1b83b8f6e7b5e0b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7aedfa8b21417aad13737b0925ba42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36d7e03-07a7-4963-981d-213869351fc4",
   "metadata": {},
   "source": [
    "## Investigate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dabc4a13-bc86-4de9-a029-381086aebf3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b650bf84-bfcf-4782-a485-7ad0e06f3b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45529290-e913-43dc-a357-6608f1712712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3638a014-bf26-4bf4-9f38-488fb5519a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94698ad1-5698-4e98-9daa-05b66a17637a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Rudder was most recently senior vice president for the Developer & Platform Evangelism Business .',\n",
       " 'sentence2': 'Senior Vice President Eric Rudder , formerly head of the Developer and Platform Evangelism unit , will lead the new entity .',\n",
       " 'label': 0,\n",
       " 'idx': 16}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46af87c6-e109-452a-b7bb-3fdeb853c1df",
   "metadata": {},
   "source": [
    "## Preprocess sentences as pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdffe070-95e5-4574-bbe4-597246ad1d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_wrong = tokenizer(\n",
    "    [raw_train_dataset[0][\"sentence1\"], raw_train_dataset[0][\"sentence2\"]]\n",
    ")\n",
    "inputs = tokenizer(\n",
    "    raw_train_dataset[0][\"sentence1\"],\n",
    "    raw_train_dataset[0][\"sentence2\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27350174-74e4-4e73-8325-c4520eecc97e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102], [101, 7727, 2000, 2032, 2004, 2069, 1000, 1996, 7409, 1000, 1010, 2572, 3217, 5831, 5496, 2010, 2567, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd8c4238-4554-4cf0-b918-f3ecb17826f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 7727, 2000, 2032, 2004, 2069, 1000, 1996, 7409, 1000, 1010, 2572, 3217, 5831, 5496, 2010, 2567, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba163e3-6ceb-4da7-8d41-9df955347fc2",
   "metadata": {},
   "source": [
    "[CLS] sentence 1 [SEP] sentence 2 [SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96544981-bcd6-49a9-b53a-de67e5d3c3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'am', '##ro', '##zi', 'accused', 'his', 'brother', ',', 'whom', 'he', 'called', '\"', 'the', 'witness', '\"', ',', 'of', 'deliberately', 'di', '##stor', '##ting', 'his', 'evidence', '.', '[SEP]', 'referring', 'to', 'him', 'as', 'only', '\"', 'the', 'witness', '\"', ',', 'am', '##ro', '##zi', 'accused', 'his', 'brother', 'of', 'deliberately', 'di', '##stor', '##ting', 'his', 'evidence', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a04679-4f46-46e1-9d3a-bf6c8fe47a43",
   "metadata": {},
   "source": [
    "[[CLS] sentence 1 [SEP]], [[CLS] sentence 2 [SEP]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "267e3324-fa9e-448f-a421-c0788244ceee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[CLS]', 'am', '##ro', '##zi', 'accused', 'his', 'brother', ',', 'whom', 'he', 'called', '\"', 'the', 'witness', '\"', ',', 'of', 'deliberately', 'di', '##stor', '##ting', 'his', 'evidence', '.', '[SEP]'], ['[CLS]', 'referring', 'to', 'him', 'as', 'only', '\"', 'the', 'witness', '\"', ',', 'am', '##ro', '##zi', 'accused', 'his', 'brother', 'of', 'deliberately', 'di', '##stor', '##ting', 'his', 'evidence', '.', '[SEP]']]\n"
     ]
    }
   ],
   "source": [
    "print(list(map(tokenizer.convert_ids_to_tokens, inputs_wrong[\"input_ids\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0a8e34f-f561-4c64-bbd2-d4b64bb5a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])\n",
    "token_type_ids = inputs[\"token_type_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dde6d2eb-7272-454f-9620-a1f26f7811c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '[CLS]'),\n",
       " (0, 'am'),\n",
       " (0, '##ro'),\n",
       " (0, '##zi'),\n",
       " (0, 'accused'),\n",
       " (0, 'his'),\n",
       " (0, 'brother'),\n",
       " (0, ','),\n",
       " (0, 'whom'),\n",
       " (0, 'he'),\n",
       " (0, 'called'),\n",
       " (0, '\"'),\n",
       " (0, 'the'),\n",
       " (0, 'witness'),\n",
       " (0, '\"'),\n",
       " (0, ','),\n",
       " (0, 'of'),\n",
       " (0, 'deliberately'),\n",
       " (0, 'di'),\n",
       " (0, '##stor'),\n",
       " (0, '##ting'),\n",
       " (0, 'his'),\n",
       " (0, 'evidence'),\n",
       " (0, '.'),\n",
       " (0, '[SEP]'),\n",
       " (1, 'referring'),\n",
       " (1, 'to'),\n",
       " (1, 'him'),\n",
       " (1, 'as'),\n",
       " (1, 'only'),\n",
       " (1, '\"'),\n",
       " (1, 'the'),\n",
       " (1, 'witness'),\n",
       " (1, '\"'),\n",
       " (1, ','),\n",
       " (1, 'am'),\n",
       " (1, '##ro'),\n",
       " (1, '##zi'),\n",
       " (1, 'accused'),\n",
       " (1, 'his'),\n",
       " (1, 'brother'),\n",
       " (1, 'of'),\n",
       " (1, 'deliberately'),\n",
       " (1, 'di'),\n",
       " (1, '##stor'),\n",
       " (1, '##ting'),\n",
       " (1, 'his'),\n",
       " (1, 'evidence'),\n",
       " (1, '.'),\n",
       " (1, '[SEP]')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(token_type_ids, tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baafa068-af3f-4317-b42f-6c56e721fc3e",
   "metadata": {},
   "source": [
    "# Tokenize the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cb6825-38e9-46ae-bda8-68cad42adeac",
   "metadata": {},
   "source": [
    "## Tokenize the dataset and save the outputs as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b2e7763e-0e14-4320-ac8e-bcdf83a6d605",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets[\"train\"][\"sentence1\"],\n",
    "    raw_datasets[\"train\"][\"sentence2\"],\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970afd2e-a84d-40d9-9486-09a766162363",
   "metadata": {},
   "source": [
    "## Tokenize the dataset using batched processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6eeafd26-04ec-45cb-841a-28103fd96761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "44a98079-25b0-48cf-85d4-f2be9a2fe711",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3cd89968-f3f7-4fd2-95de-46b765878734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "22099707-3651-4be0-a6ec-8d4cef83f697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d202923-287e-4028-b4cf-72333c9a412e",
   "metadata": {},
   "source": [
    "## Dynamic padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "84215b46-19eb-442c-b06f-afbe89fbec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "79083637-0bd7-43a5-b363-c75c99c6e794",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = tokenized_datasets[\"train\"][:8]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fdc187b4-401f-4b09-898b-07678d0aff2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 59, 47, 67, 59, 50, 62, 32]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in samples[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "28ecb5ba-5d68-4a2c-8b04-ce4e63d58961",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = data_collator(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b5e2de63-a166-475b-abc6-8cfbc71731b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 67]),\n",
       " 'token_type_ids': torch.Size([8, 67]),\n",
       " 'attention_mask': torch.Size([8, 67]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3050c50f-df2d-4c15-a558-c7361f714564",
   "metadata": {},
   "source": [
    "### Verify behavior is different for different batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b2c81fd6-21dc-4852-b9d6-39a427909a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': torch.Size([8, 67]), 'token_type_ids': torch.Size([8, 67]), 'attention_mask': torch.Size([8, 67]), 'labels': torch.Size([8])}\n",
      "{'input_ids': torch.Size([8, 61]), 'token_type_ids': torch.Size([8, 61]), 'attention_mask': torch.Size([8, 61]), 'labels': torch.Size([8])}\n",
      "{'input_ids': torch.Size([8, 79]), 'token_type_ids': torch.Size([8, 79]), 'attention_mask': torch.Size([8, 79]), 'labels': torch.Size([8])}\n",
      "{'input_ids': torch.Size([8, 68]), 'token_type_ids': torch.Size([8, 68]), 'attention_mask': torch.Size([8, 68]), 'labels': torch.Size([8])}\n",
      "{'input_ids': torch.Size([8, 73]), 'token_type_ids': torch.Size([8, 73]), 'attention_mask': torch.Size([8, 73]), 'labels': torch.Size([8])}\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 32 + 1, 8):\n",
    "    samples = tokenized_datasets[\"train\"][i:i + 8]\n",
    "    samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "    batch = data_collator(samples)\n",
    "    print({k: v.shape for k, v in batch.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b908ba-1752-4459-b1a4-fd36d1df441c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
