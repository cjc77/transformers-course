{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2266584-0879-4dda-97dd-28c1355ed670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e514c98-aef0-478f-946c-8f8665324ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "824a47b7-095e-40c5-926b-8fe6b92ea520",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"xlnet-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29ab719b-381e-4391-9f44-62b8e62f7404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f98a22f45ebc4fb9a7a2ebbe474189a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd3849ea0f0492ca2fbfe6d30c98d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6b08844c0e46f8921868c32e1d5c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.38M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69463aa5-4893-4ff6-830d-fe0f59a4b7cc",
   "metadata": {},
   "source": [
    "# Pre-tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747ef4ff-4309-4b6c-b6ae-9d836aeb4c70",
   "metadata": {},
   "source": [
    "## Compute word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "076824f5-fd08-498a-a825-8b601131512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freqs = defaultdict(lambda: 0)\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [w for w, _ in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bfc17c3-ec13-4ee6-b2a3-b6663ef1bc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'▁This': 3,\n",
       "             '▁is': 2,\n",
       "             '▁the': 1,\n",
       "             '▁Hugging': 1,\n",
       "             '▁Face': 1,\n",
       "             '▁Course.': 1,\n",
       "             '▁chapter': 1,\n",
       "             '▁about': 1,\n",
       "             '▁tokenization.': 1,\n",
       "             '▁section': 1,\n",
       "             '▁shows': 1,\n",
       "             '▁several': 1,\n",
       "             '▁tokenizer': 1,\n",
       "             '▁algorithms.': 1,\n",
       "             '▁Hopefully,': 1,\n",
       "             '▁you': 1,\n",
       "             '▁will': 1,\n",
       "             '▁be': 1,\n",
       "             '▁able': 1,\n",
       "             '▁to': 1,\n",
       "             '▁understand': 1,\n",
       "             '▁how': 1,\n",
       "             '▁they': 1,\n",
       "             '▁are': 1,\n",
       "             '▁trained': 1,\n",
       "             '▁and': 1,\n",
       "             '▁generate': 1,\n",
       "             '▁tokens.': 1})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb83cb88-a756-4ddb-926d-3d1ec9cc5154",
   "metadata": {},
   "source": [
    "## Increase initial vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f97dd05-f96b-48a8-a273-8c604c712fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_freqs = defaultdict(lambda: 0)\n",
    "subword_freqs = defaultdict(lambda: 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beb80d55-fc3a-4ae5-9a67-874f793bd005",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, freq in word_freqs.items():\n",
    "    for i in range(len(word)):\n",
    "        char_freqs[word[i]] += freq\n",
    "        # Loop through subwords of length >= 2\n",
    "        for j in range(i + 2, len(word) + 1):\n",
    "            subword_freqs[word[i:j]] += freq\n",
    "# Sort subwords by frequency\n",
    "sorted_subwords = sorted(subword_freqs.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fe3d932-68b5-4c83-92e3-6c336a97df79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁t', 7),\n",
       " ('is', 5),\n",
       " ('er', 5),\n",
       " ('▁a', 5),\n",
       " ('▁to', 4),\n",
       " ('to', 4),\n",
       " ('en', 4),\n",
       " ('▁T', 3),\n",
       " ('▁Th', 3),\n",
       " ('▁Thi', 3)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_subwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f62a0e2-d848-48fc-8bb3-c3d4755f4bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freqs = list(char_freqs.items()) + sorted_subwords[:300 - len(char_freqs)]\n",
    "token_freqs = {token: freq for token, freq in token_freqs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4bba17-17f0-4bca-aac6-ffc2e9d260e8",
   "metadata": {},
   "source": [
    "## Compute frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a09398c4-add7-44ff-8c5d-493ca9438659",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "model = {token: -np.log(freq / total_sum) for token, freq in token_freqs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f119a6-f062-4aeb-848d-4928626c1ef9",
   "metadata": {},
   "source": [
    "## Encode words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "090116c0-09ea-4f89-a849-e1a69b67680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word(word, model):\n",
    "    best_segmentations = [{\"start\": 0, \"score\": 1}] + [\n",
    "        {\"start\": None, \"score\": None} for _ in range(len(word))\n",
    "    ]\n",
    "    for start_idx in range(len(word)):\n",
    "        best_score_at_start = best_segmentations[start_idx][\"score\"]\n",
    "        for end_idx in range(start_idx + 1, len(word) + 1):\n",
    "            token = word[start_idx:end_idx]\n",
    "            if token in model and best_score_at_start is not None:\n",
    "                score = model[token] + best_score_at_start\n",
    "                # Better segmentation ending at end_idx?\n",
    "                if (\n",
    "                    best_segmentations[end_idx][\"score\"] is None or\n",
    "                    best_segmentations[end_idx][\"score\"] > score\n",
    "                ):\n",
    "                    best_segmentations[end_idx] = {\"start\": start_idx, \"score\": score}\n",
    "\n",
    "    segmentation = best_segmentations[-1]\n",
    "    if segmentation[\"score\"] is None:\n",
    "        # Didn't find tokenization\n",
    "        return [\"<unk>\"], None\n",
    "    score = segmentation[\"score\"]\n",
    "    start = segmentation[\"start\"]\n",
    "    end = len(word)\n",
    "    tokens = []\n",
    "    while start != 0:\n",
    "        tokens.insert(0, word[start:end])\n",
    "        next_start = best_segmentations[start][\"start\"]\n",
    "        end = start\n",
    "        start = next_start\n",
    "    tokens.insert(0, word[start:end])\n",
    "    return tokens, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfc40d3a-7455-4619-9527-58b4591c9eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402)\n",
      "(['This'], 6.288267030694535)\n"
     ]
    }
   ],
   "source": [
    "print(encode_word(\"Hopefully\", model))\n",
    "print(encode_word(\"This\", model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5e870ef-a619-4c72-b749-ceb65f6985e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('▁This', 3), ('▁is', 2), ('▁the', 1), ('▁Hugging', 1), ('▁Face', 1), ('▁Course.', 1), ('▁chapter', 1), ('▁about', 1), ('▁tokenization.', 1), ('▁section', 1), ('▁shows', 1), ('▁several', 1), ('▁tokenizer', 1), ('▁algorithms.', 1), ('▁Hopefully,', 1), ('▁you', 1), ('▁will', 1), ('▁be', 1), ('▁able', 1), ('▁to', 1), ('▁understand', 1), ('▁how', 1), ('▁they', 1), ('▁are', 1), ('▁trained', 1), ('▁and', 1), ('▁generate', 1), ('▁tokens.', 1)])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freqs.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cac403a-387f-45c8-b3c5-f0f2ca15cb52",
   "metadata": {},
   "source": [
    "## Compute the loss of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13dabd8d-eb61-43e7-81ed-8012650c1b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model):\n",
    "    loss = 0\n",
    "    for word, freq in word_freqs.items():\n",
    "        _, word_loss = encode_word(word, model)\n",
    "        loss += freq * word_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3a20f78-5f52-4eda-887d-68ac3f0373d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413.10377642940875\n"
     ]
    }
   ],
   "source": [
    "print(compute_loss(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a91d51e-9c1e-4543-85b7-33e59bb30020",
   "metadata": {},
   "source": [
    "## Compute the scores for each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6b3607b-fa05-4647-a770-6e2fbc9c10ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(model):\n",
    "    scores = {}\n",
    "    model_loss = compute_loss(model)\n",
    "    for token, score in model.items():\n",
    "        # Keep tokens of length 1\n",
    "        if len(token) == 1:\n",
    "            continue\n",
    "        model_without_token = deepcopy(model)\n",
    "        _ = model_without_token.pop(token)\n",
    "        scores[token] = compute_loss(model_without_token) - model_loss\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe2f0d76-b22a-473e-8771-f3f391e3efb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.376412403623874\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "scores = compute_scores(model)\n",
    "print(scores[\"ll\"])\n",
    "print(scores[\"his\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32907609-34ee-4335-8d4e-d0ef2ef775c1",
   "metadata": {},
   "source": [
    "## Prune the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6aa9299f-3cb5-4d38-995b-ce357b34a646",
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_to_remove = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44b8c3a6-d705-479f-bd9d-d083e51a129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(model) > 100:\n",
    "    scores = compute_scores(model)\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1])\n",
    "    # Remove percent_to_remove tokens with lowest scores\n",
    "    for i in range(int(len(model) * pct_to_remove)):\n",
    "        _ = token_freqs.pop(sorted_scores[i][0])\n",
    "    total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "    model = {token: -np.log(freq / total_sum) for token, freq in token_freqs.items()}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed0cbc1-1272-4e5a-9010-2de89347cdad",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b5972a43-49e1-4a75-86e3-65b35d7c02cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, model):\n",
    "    words_with_offsets = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, _ in words_with_offsets]\n",
    "    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]\n",
    "    return sum(encoded_words, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a603045a-a4f8-4539-a2fe-bf81667a22f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁This',\n",
       " '▁is',\n",
       " '▁the',\n",
       " '▁Hugging',\n",
       " '▁Face',\n",
       " '▁',\n",
       " 'c',\n",
       " 'ou',\n",
       " 'r',\n",
       " 's',\n",
       " 'e',\n",
       " '.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"This is the Hugging Face course.\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29990ac1-1704-4c6f-be19-c6d14255acca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
