{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3b2fe321-f8b5-450b-ba67-692a699ba0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer\n",
    ")\n",
    "from transformers import PreTrainedTokenizerFast, BertTokenizerFast, GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3147d5a-a8e9-4a85-bc79-a4f5cf0588a8",
   "metadata": {},
   "source": [
    "# Retrieve a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f0ee095-7320-4559-87f3-a0fe6b2ab8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0264c6e-1ae9-40e6-8215-9445ee84ee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i:i + 1000][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b72e54b-f223-4340-9148-a24a0fa9ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../data/wikitext-2/wikitext-2.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bee365c5-7b17-40fc-9bc0-8d03a22196fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filedir = '/'.join(filepath.split('/')[:-1])\n",
    "if not os.path.exists(filedir):\n",
    "    os.makedirs(filedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18c824dd-4fdf-4363-b41a-8fdda3c38704",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(filepath):\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\", ) as f:\n",
    "        for i in range(len(dataset)):\n",
    "            f.write(dataset[i][\"text\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8555154-a92f-41c9-aafb-3576cdc0ee3d",
   "metadata": {},
   "source": [
    "# WordPiece tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f24bc76-b486-43ef-b188-32784cdfb9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7ef1f0-617f-43a6-b581-a6e034193e03",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04617ed5-2866-48d8-b8d7-739ae7a608b7",
   "metadata": {},
   "source": [
    "### Replicate `bert-base-uncased` normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54ad3017-fc94-46d2-8244-fec124c2f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87f07620-6388-4cbb-b2b9-5af7732f2f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.normalizer.normalize_str(u\"\\u0085\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c03b1a7-67e0-47d4-b90a-7bd641f209c4",
   "metadata": {},
   "source": [
    "### Build normalizer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2cb890b-4fea-4906-9ebd-76de061bb20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22ae4d26-4b88-43ac-894c-c508dfd3ca58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are u?\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ae3c953-feda-43fc-9b71-3467b1c47cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x85'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: this can be fixed by adding a couple of complicated\n",
    "# regex statements to the sequence\n",
    "tokenizer.normalizer.normalize_str(u\"\\u0085\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db43a346-fcaf-4b0d-8d0b-90c5036d3a19",
   "metadata": {},
   "source": [
    "## Pre-tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4870da97-2cd8-42f9-82ac-8ac1dbf0b536",
   "metadata": {},
   "source": [
    "### Replicate `bert-base-uncased` pre-tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14709f44-e652-4ef8-9565-935a1e1d9d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd24e9f-1761-4ecc-98d4-740f49aef484",
   "metadata": {},
   "source": [
    "### Build pre-tokenizer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f428c5c2-87d0-44db-90a8-5881243aa62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits on whitespace and punctuation\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "289b9cfa-56fe-42ea-afbc-d7c1a9bf25fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'\", (3, 4)),\n",
       " ('s', (4, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre', (14, 17)),\n",
       " ('-', (17, 18)),\n",
       " ('tokenizer', (18, 27)),\n",
       " ('.', (27, 28))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e599676-06d4-47ea-91ef-adc295ea6230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits on whitespace only\n",
    "pre_tokenizer = pre_tokenizers.WhitespaceSplit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89e2768f-ca5a-45e2-9905-70b570c54b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Let's\", (0, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre-tokenizer.', (14, 28))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59dc480e-5585-42a1-ac80-3f06b1c00446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'\", (3, 4)),\n",
       " ('s', (4, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre', (14, 17)),\n",
       " ('-', (17, 18)),\n",
       " ('tokenizer', (18, 27)),\n",
       " ('.', (27, 28))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split on whitespace and pupnctuation, using Sequence\n",
    "pre_tokenizer = pre_tokenizers.Sequence(\n",
    "    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]\n",
    ")\n",
    "pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948e4bf3-9f3b-4c5a-b3bf-b63bcf7c4cfc",
   "metadata": {},
   "source": [
    "## Tokenizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dc2972e-d668-4277-b267-e8c77d5417a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "vocab_size = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a785b6ae-c69f-4212-ad39-ee976753ee9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.WordPieceTrainer(vocab_size=vocab_size, special_tokens=special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dae058b-7c96-4abf-8a70-47010d401285",
   "metadata": {},
   "source": [
    "### Train the model from the iterator defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "666fe36c-9ac7-4125-8f30-bf07a28c7e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75d17e2-c7a0-4d67-860a-c7edfec7a61f",
   "metadata": {},
   "source": [
    "### Train the model from the text file directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "329fa1dd-d855-485a-923a-d2248af652c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.model = models.WordPiece(unk_token=\"[UNK]\")\n",
    "tokenizer.train([filepath], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9184346e-177d-4053-96f5-bbb646199ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be1a2876-adf5-4f97-a2c4-e0e3df90b9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']\n"
     ]
    }
   ],
   "source": [
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f270c1c5-0173-41a3-b3ae-4600574e6306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=9, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "print(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd94ed4-e9b4-4fa3-bb14-19ccc0b86264",
   "metadata": {},
   "source": [
    "## Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c433c986-cd55-4d07-a14b-752f2f8f9301",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "899ec8f6-f4f3-4968-899b-7c33a38c8579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n"
     ]
    }
   ],
   "source": [
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697e9160-a402-4875-8b9f-7a8095d15e70",
   "metadata": {},
   "source": [
    "### Classic BERT template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "909cc9bd-072d-4328-8de6-b45014024993",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6c46a8cb-175d-4520-8364-c4b583bf810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7709820b-eac8-4665-bc00-e2d4252e1368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "627f7b73-bb6f-43a3-b159-0b39ecd63581",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b98e47dd-ee48-4112-a56d-a0e98b330afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '...', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd35519d-6b28-447c-81f1-bad04e2ce5bf",
   "metadata": {},
   "source": [
    "### Include a decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "faa77728-53d9-4efb-98e6-5db9ab2c400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ab47c01-4e14-476f-97f8-a6bb77ea9cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"let ' s test this tokenizer... on a pair of sentences.\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a94d00-2a0f-4748-bcf7-c466514075a1",
   "metadata": {},
   "source": [
    "## Save tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9a74fe95-2ac1-46ba-8ad4-4d1d04df1e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = \"../temp/06_tokenizer_block_by_block/tokenizer.json\"\n",
    "tokenizer_dir = \"/\".join(tokenizer_path.split('/')[:-1])\n",
    "if not os.path.exists(tokenizer_dir):\n",
    "    os.makedirs(tokenizer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "54c2eadc-7050-456a-aed6-64289f097b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(tokenizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e347961-b919-4f4a-a834-c6292222b4ae",
   "metadata": {},
   "source": [
    "## Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "acc72654-4d17-4cf2-a58e-648ac134db32",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer = Tokenizer.from_file(tokenizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed84b71-6d5c-4998-b1f1-bd637822ed02",
   "metadata": {},
   "source": [
    "## Wrap tokenizer in `PreTrainedTokenizerFast` class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d524ad97-a138-499c-a6ad-47d3e8f102cd",
   "metadata": {},
   "source": [
    "### Generic wrapper\n",
    "Use this if your tokenizer doesn't correspond directly to an existing model (e.g., BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b4b17b5f-8eab-477b-a4c1-33e68412779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,  # Could pass tokenizer file instead\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f24c9b-76cf-4961-874c-3f8890062489",
   "metadata": {},
   "source": [
    "### Specific wrapper\n",
    "Use this if your tokenizer corresponds directly to an existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "651f2803-df3a-40dd-9399-1c178dee214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer = BertTokenizerFast(\n",
    "    tokenizer_object=tokenizer  # Could pass tokenizer file instead\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edd9f0f-b0a0-455b-90da-e84732c898a0",
   "metadata": {},
   "source": [
    "# BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "97983b5d-641b-4c61-a537-c375a8863464",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.BPE())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8b47d5-3050-46fd-b1e4-7a5329268f8e",
   "metadata": {},
   "source": [
    "## No normalizer for GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3befbf21-6710-4637-97ed-a5f47534d910",
   "metadata": {},
   "source": [
    "## Pre-tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ae660f6d-3a81-444d-b418-45f9fad49922",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1ba329ca-788a-4938-a382-d647c326ed5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'s\", (3, 5)),\n",
       " ('Ġtest', (5, 10)),\n",
       " ('Ġpre', (10, 14)),\n",
       " ('-', (14, 15)),\n",
       " ('tokenization', (15, 27)),\n",
       " ('!', (27, 28))]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test pre-tokenization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeedfcd6-434a-4319-9b8b-10fb83fff8b5",
   "metadata": {},
   "source": [
    "## Tokenizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1c880fba-e231-4c15-b495-2a9ecd2ee442",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=[\"<|endoftext|>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ff9279-1fd7-4965-89ef-1247b5d87eef",
   "metadata": {},
   "source": [
    "### Train the model from the iterator defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "84523cf8-2d67-4a94-b8bf-3ec9416da073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee1e54f-fe3b-40cf-ab3f-f6be4bc23b66",
   "metadata": {},
   "source": [
    "### Train the model from the text file directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "50db652d-1966-4dfd-9377-42a0ceee608d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.model = models.BPE()\n",
    "tokenizer.train([filepath], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2cbfb81f-e817-479a-9968-99cbbfb7525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a2d59b6b-79b5-4af6-9600-64c8c9f0981c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L', 'et', \"'\", 's', 'Ġtest', 'Ġthis', 'Ġto', 'ken', 'izer', '.']\n"
     ]
    }
   ],
   "source": [
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b57c582-c741-4bdf-a5c1-268f76f06c45",
   "metadata": {},
   "source": [
    "## Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0a7e62f7-693e-4dcc-bd27-539e43132a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "82facabe-026d-4be3-951c-217a52918c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' test'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Let's test this tokenizer.\"\n",
    "encoding = tokenizer.encode(sentence)\n",
    "start, end = encoding.offsets[4]\n",
    "sentence[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fb430a-6150-443e-a7a8-f895e81ea827",
   "metadata": {},
   "source": [
    "### Include a decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "12f2d379-838c-4e77-bb22-c22c420f511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.ByteLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "32cb679f-83c6-45f4-8c60-349c73063f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let's test this tokenizer.\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13be9033-d8cb-4cf0-a844-2e953efb4165",
   "metadata": {},
   "source": [
    "## Wrap tokenizer in `PreTrainedTokenizerFast` class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0ea7a0-0ec7-41f7-8fc8-377759238f00",
   "metadata": {},
   "source": [
    "### Generic wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2cb2675d-96f2-407a-9846-d7104e3f6fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"<|endoftext|>\",\n",
    "    eos_token=\"<|endoftext|>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e2db3e-708c-43bf-9cc0-5935cbd4879d",
   "metadata": {},
   "source": [
    "### Specific wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fc183f3d-c17c-4b92-a400-cfb24acbe2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd453cf6-93d4-4aa6-8120-cfdc4137ad55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
