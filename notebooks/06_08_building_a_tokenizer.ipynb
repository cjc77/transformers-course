{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3b2fe321-f8b5-450b-ba67-692a699ba0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    "    Regex\n",
    ")\n",
    "from transformers import PreTrainedTokenizerFast, BertTokenizerFast, GPT2TokenizerFast, XLNetTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3147d5a-a8e9-4a85-bc79-a4f5cf0588a8",
   "metadata": {},
   "source": [
    "# Retrieve a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f0ee095-7320-4559-87f3-a0fe6b2ab8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0264c6e-1ae9-40e6-8215-9445ee84ee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i:i + 1000][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b72e54b-f223-4340-9148-a24a0fa9ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../data/wikitext-2/wikitext-2.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bee365c5-7b17-40fc-9bc0-8d03a22196fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filedir = '/'.join(filepath.split('/')[:-1])\n",
    "if not os.path.exists(filedir):\n",
    "    os.makedirs(filedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18c824dd-4fdf-4363-b41a-8fdda3c38704",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(filepath):\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\", ) as f:\n",
    "        for i in range(len(dataset)):\n",
    "            f.write(dataset[i][\"text\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8555154-a92f-41c9-aafb-3576cdc0ee3d",
   "metadata": {},
   "source": [
    "# WordPiece tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f24bc76-b486-43ef-b188-32784cdfb9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7ef1f0-617f-43a6-b581-a6e034193e03",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04617ed5-2866-48d8-b8d7-739ae7a608b7",
   "metadata": {},
   "source": [
    "### Replicate `bert-base-uncased` normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54ad3017-fc94-46d2-8244-fec124c2f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87f07620-6388-4cbb-b2b9-5af7732f2f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.normalizer.normalize_str(u\"\\u0085\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c03b1a7-67e0-47d4-b90a-7bd641f209c4",
   "metadata": {},
   "source": [
    "### Build normalizer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2cb890b-4fea-4906-9ebd-76de061bb20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22ae4d26-4b88-43ac-894c-c508dfd3ca58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are u?\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ae3c953-feda-43fc-9b71-3467b1c47cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x85'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: this can be fixed by adding a couple of complicated\n",
    "# regex statements to the sequence\n",
    "tokenizer.normalizer.normalize_str(u\"\\u0085\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db43a346-fcaf-4b0d-8d0b-90c5036d3a19",
   "metadata": {},
   "source": [
    "## Pre-tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4870da97-2cd8-42f9-82ac-8ac1dbf0b536",
   "metadata": {},
   "source": [
    "### Replicate `bert-base-uncased` pre-tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14709f44-e652-4ef8-9565-935a1e1d9d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd24e9f-1761-4ecc-98d4-740f49aef484",
   "metadata": {},
   "source": [
    "### Build pre-tokenizer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f428c5c2-87d0-44db-90a8-5881243aa62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits on whitespace and punctuation\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "289b9cfa-56fe-42ea-afbc-d7c1a9bf25fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'\", (3, 4)),\n",
       " ('s', (4, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre', (14, 17)),\n",
       " ('-', (17, 18)),\n",
       " ('tokenizer', (18, 27)),\n",
       " ('.', (27, 28))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e599676-06d4-47ea-91ef-adc295ea6230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits on whitespace only\n",
    "pre_tokenizer = pre_tokenizers.WhitespaceSplit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89e2768f-ca5a-45e2-9905-70b570c54b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Let's\", (0, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre-tokenizer.', (14, 28))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59dc480e-5585-42a1-ac80-3f06b1c00446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'\", (3, 4)),\n",
       " ('s', (4, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre', (14, 17)),\n",
       " ('-', (17, 18)),\n",
       " ('tokenizer', (18, 27)),\n",
       " ('.', (27, 28))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split on whitespace and pupnctuation, using Sequence\n",
    "pre_tokenizer = pre_tokenizers.Sequence(\n",
    "    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]\n",
    ")\n",
    "pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948e4bf3-9f3b-4c5a-b3bf-b63bcf7c4cfc",
   "metadata": {},
   "source": [
    "## Tokenizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dc2972e-d668-4277-b267-e8c77d5417a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "vocab_size = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a785b6ae-c69f-4212-ad39-ee976753ee9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.WordPieceTrainer(vocab_size=vocab_size, special_tokens=special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dae058b-7c96-4abf-8a70-47010d401285",
   "metadata": {},
   "source": [
    "### Train the model from the iterator defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "666fe36c-9ac7-4125-8f30-bf07a28c7e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75d17e2-c7a0-4d67-860a-c7edfec7a61f",
   "metadata": {},
   "source": [
    "### Train the model from the text file directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "329fa1dd-d855-485a-923a-d2248af652c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.model = models.WordPiece(unk_token=\"[UNK]\")\n",
    "tokenizer.train([filepath], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9184346e-177d-4053-96f5-bbb646199ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be1a2876-adf5-4f97-a2c4-e0e3df90b9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']\n"
     ]
    }
   ],
   "source": [
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f270c1c5-0173-41a3-b3ae-4600574e6306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=9, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "print(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd94ed4-e9b4-4fa3-bb14-19ccc0b86264",
   "metadata": {},
   "source": [
    "## Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c433c986-cd55-4d07-a14b-752f2f8f9301",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "899ec8f6-f4f3-4968-899b-7c33a38c8579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n"
     ]
    }
   ],
   "source": [
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697e9160-a402-4875-8b9f-7a8095d15e70",
   "metadata": {},
   "source": [
    "### Classic BERT template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "909cc9bd-072d-4328-8de6-b45014024993",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6c46a8cb-175d-4520-8364-c4b583bf810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7709820b-eac8-4665-bc00-e2d4252e1368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "627f7b73-bb6f-43a3-b159-0b39ecd63581",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b98e47dd-ee48-4112-a56d-a0e98b330afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '...', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd35519d-6b28-447c-81f1-bad04e2ce5bf",
   "metadata": {},
   "source": [
    "### Include a decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "faa77728-53d9-4efb-98e6-5db9ab2c400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ab47c01-4e14-476f-97f8-a6bb77ea9cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"let ' s test this tokenizer... on a pair of sentences.\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a94d00-2a0f-4748-bcf7-c466514075a1",
   "metadata": {},
   "source": [
    "## Save tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9a74fe95-2ac1-46ba-8ad4-4d1d04df1e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = \"../temp/06_tokenizer_block_by_block/tokenizer.json\"\n",
    "tokenizer_dir = \"/\".join(tokenizer_path.split('/')[:-1])\n",
    "if not os.path.exists(tokenizer_dir):\n",
    "    os.makedirs(tokenizer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "54c2eadc-7050-456a-aed6-64289f097b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(tokenizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e347961-b919-4f4a-a834-c6292222b4ae",
   "metadata": {},
   "source": [
    "## Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "acc72654-4d17-4cf2-a58e-648ac134db32",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer = Tokenizer.from_file(tokenizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed84b71-6d5c-4998-b1f1-bd637822ed02",
   "metadata": {},
   "source": [
    "## Wrap tokenizer in `PreTrainedTokenizerFast` class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d524ad97-a138-499c-a6ad-47d3e8f102cd",
   "metadata": {},
   "source": [
    "### Generic wrapper\n",
    "Use this if your tokenizer doesn't correspond directly to an existing model (e.g., BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b4b17b5f-8eab-477b-a4c1-33e68412779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,  # Could pass tokenizer file instead\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f24c9b-76cf-4961-874c-3f8890062489",
   "metadata": {},
   "source": [
    "### Specific wrapper\n",
    "Use this if your tokenizer corresponds directly to an existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "651f2803-df3a-40dd-9399-1c178dee214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer = BertTokenizerFast(\n",
    "    tokenizer_object=tokenizer  # Could pass tokenizer file instead\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edd9f0f-b0a0-455b-90da-e84732c898a0",
   "metadata": {},
   "source": [
    "# BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "97983b5d-641b-4c61-a537-c375a8863464",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.BPE())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8b47d5-3050-46fd-b1e4-7a5329268f8e",
   "metadata": {},
   "source": [
    "## No normalizer for GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3befbf21-6710-4637-97ed-a5f47534d910",
   "metadata": {},
   "source": [
    "## Pre-tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ae660f6d-3a81-444d-b418-45f9fad49922",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1ba329ca-788a-4938-a382-d647c326ed5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'s\", (3, 5)),\n",
       " ('Ġtest', (5, 10)),\n",
       " ('Ġpre', (10, 14)),\n",
       " ('-', (14, 15)),\n",
       " ('tokenization', (15, 27)),\n",
       " ('!', (27, 28))]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test pre-tokenization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeedfcd6-434a-4319-9b8b-10fb83fff8b5",
   "metadata": {},
   "source": [
    "## Tokenizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1c880fba-e231-4c15-b495-2a9ecd2ee442",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=[\"<|endoftext|>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ff9279-1fd7-4965-89ef-1247b5d87eef",
   "metadata": {},
   "source": [
    "### Train the model from the iterator defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "84523cf8-2d67-4a94-b8bf-3ec9416da073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee1e54f-fe3b-40cf-ab3f-f6be4bc23b66",
   "metadata": {},
   "source": [
    "### Train the model from the text file directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "50db652d-1966-4dfd-9377-42a0ceee608d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.model = models.BPE()\n",
    "tokenizer.train([filepath], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2cbfb81f-e817-479a-9968-99cbbfb7525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a2d59b6b-79b5-4af6-9600-64c8c9f0981c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L', 'et', \"'\", 's', 'Ġtest', 'Ġthis', 'Ġto', 'ken', 'izer', '.']\n"
     ]
    }
   ],
   "source": [
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b57c582-c741-4bdf-a5c1-268f76f06c45",
   "metadata": {},
   "source": [
    "## Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0a7e62f7-693e-4dcc-bd27-539e43132a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "82facabe-026d-4be3-951c-217a52918c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' test'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Let's test this tokenizer.\"\n",
    "encoding = tokenizer.encode(sentence)\n",
    "start, end = encoding.offsets[4]\n",
    "sentence[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fb430a-6150-443e-a7a8-f895e81ea827",
   "metadata": {},
   "source": [
    "### Include a decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "12f2d379-838c-4e77-bb22-c22c420f511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.ByteLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "32cb679f-83c6-45f4-8c60-349c73063f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let's test this tokenizer.\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13be9033-d8cb-4cf0-a844-2e953efb4165",
   "metadata": {},
   "source": [
    "## Wrap tokenizer in `PreTrainedTokenizerFast` class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0ea7a0-0ec7-41f7-8fc8-377759238f00",
   "metadata": {},
   "source": [
    "### Generic wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2cb2675d-96f2-407a-9846-d7104e3f6fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"<|endoftext|>\",\n",
    "    eos_token=\"<|endoftext|>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e2db3e-708c-43bf-9cc0-5935cbd4879d",
   "metadata": {},
   "source": [
    "### Specific wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fc183f3d-c17c-4b92-a400-cfb24acbe2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb801bf4-bfbb-47c4-b32c-3d0ab1a3f4d1",
   "metadata": {},
   "source": [
    "# Unigram tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e3741c44-45c5-403f-93cf-f915ac309e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.Unigram())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746409d4-e743-49b0-968f-910f2a32e3c9",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a0e34fa9-eec9-41cc-b031-2786bd03530e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.Sequence([\n",
    "    normalizers.Replace(\"``\", '\"'),\n",
    "    normalizers.Replace(\"''\", '\"'),\n",
    "    normalizers.NFKD(),\n",
    "    normalizers.StripAccents(),\n",
    "    normalizers.Replace(Regex(\" {2,}\"), \" \")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab441ca6-9821-461f-9983-012a9ba3bb3b",
   "metadata": {},
   "source": [
    "## Pre-tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4644b498-26bc-4e21-bae3-2583c8107602",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e9a80617-88b1-4fea-8cf2-182fdd4f0c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"▁Let's\", (0, 5)),\n",
       " ('▁test', (5, 10)),\n",
       " ('▁the', (10, 14)),\n",
       " ('▁pre-tokenizer!', (14, 29))]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test the pre-tokenizer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db27a85-70ca-451b-bc93-cee0f855c652",
   "metadata": {},
   "source": [
    "## Tokenizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b57c49bf-c888-49fc-88df-c4315f723e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"<cls>\", \"<sep>\", \"<unk>\", \"<pad>\", \"<mask>\", \"<s>\", \"</s>\"]\n",
    "trainer = trainers.UnigramTrainer(\n",
    "    vocab_size=vocab_size,\n",
    "    special_tokens=special_tokens,\n",
    "    unk_token=\"<unk>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b6f19c-0521-4f20-b578-11c1114fb736",
   "metadata": {},
   "source": [
    "### Train the model from the iterator defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4394abef-a2d9-4157-af93-2f973934103e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3453943-c3a5-431a-a302-ec1c0ea37185",
   "metadata": {},
   "source": [
    "### Train the model from the text file directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2188892f-b9a3-4fb1-9f8f-9f493b9f9885",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model = models.Unigram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "865c8091-0e53-4278-ba17-af200f8ca914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train([filepath], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d98e644c-edf5-49c1-bfef-e5a47b94b09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9cc65afe-e454-4748-8a70-a163d6f1b323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Let', \"'\", 's', '▁test', '▁this', '▁to', 'ken', 'izer', '.']\n"
     ]
    }
   ],
   "source": [
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248eef83-9998-430d-9d45-14f5bed3aa8e",
   "metadata": {},
   "source": [
    "## Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5c3aad8e-2988-4267-8e65-fa66d9909a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"<cls>\")\n",
    "sep_token_id = tokenizer.token_to_id(\"<sep>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e853879e-a58a-49ea-8305-c82d752bb080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n"
     ]
    }
   ],
   "source": [
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c8dba9-5499-477c-9fee-fdfc792db1cf",
   "metadata": {},
   "source": [
    "### Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9db2e816-5242-4c2e-b7ca-056f783769c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"$A:0 <sep>:0 <cls>:2\",\n",
    "    pair=\"$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2\",\n",
    "    special_tokens=[(\"<sep>\", sep_token_id), (\"<cls>\", cls_token_id)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6adb048b-5003-42d7-9b6b-a2a084e3aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "387a0af5-f160-42fe-a7dd-f5bfcf6a5e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Let', \"'\", 's', '▁test', '▁this', '▁to', 'ken', 'izer', '.', '.', '.', '<sep>', '▁', 'on', '▁', 'a', '▁pair', '▁of', '▁sentence', 's', '!', '<sep>', '<cls>']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a51824e1-2200-4a25-9fb9-da485d7c409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.Metaspace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ed2aea-3515-4d1d-9973-bbd88eed41b7",
   "metadata": {},
   "source": [
    "## Wrap tokenizer in `PreTrainedTokenizerFast` class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4008b10e-e594-4899-b492-6dab7adc46f9",
   "metadata": {},
   "source": [
    "### Generic wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c098debe-e04e-4641-a9d3-8147b76d1ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    cls_token=\"<cls>\",\n",
    "    sep_token=\"<sep>\",\n",
    "    mask_token=\"<mask>\",\n",
    "    padding_side=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f16903-6aa9-4cc3-91ba-e3e7f0ee58eb",
   "metadata": {},
   "source": [
    "### Specific wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e6def358-56be-4475-bf5b-c3da571c6aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee818e5-9dd7-4b08-bdf7-f24b6df2e3b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
